% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}



\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\usepackage{fontspec}
\usepackage{unicode-math}
\setmainfont{Latin Modern Roman}
\setmathfont{Latin Modern Math}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Comprehensive Data Analysis Collection},
  pdfauthor={Norah Jones},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Comprehensive Data Analysis Collection}
\author{Norah Jones}
\date{2025-11-18}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables

\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

This is a Quarto book.

To learn more about Quarto books visit
\url{https://quarto.org/docs/books}.

\bookmarksetup{startatroot}

\chapter{Introduction}\label{introduction}

This is a book created from markdown and executable code.

See Knuth (1984) for additional discussion of literate programming.

\part{Optimization Methods}

\chapter{Introduction to Optimization for Data
Science}\label{introduction-to-optimization-for-data-science}

Fundamentals of Continuous Optimization

\hfill\break

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, breakable, colframe=quarto-callout-note-color-frame, colback=white, rightrule=.15mm, bottomtitle=1mm, leftrule=.75mm, colbacktitle=quarto-callout-note-color!10!white, left=2mm, toptitle=1mm, titlerule=0mm, bottomrule=.15mm, title={\textbf{Chapter Overview}}, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, coltitle=black]

This chapter introduces the fundamentals of \textbf{continuous
optimization algorithms} for data science applications. We explore how
machine learning, statistics, and data analysis problems can be
formulated as optimization challenges.

\textbf{Key Topics Covered:} - Data analysis through optimization lens -
Classical optimization problems (Least Squares, LASSO) - Matrix
factorization and low-rank problems\\
- Machine learning formulations (SVM, Logistic Regression) - Deep
learning optimization challenges

\end{tcolorbox}

\section{Introduction}\label{sec-introduction}

This book focuses on the \textbf{fundamentals of algorithms} for solving
continuous optimization problems, which involve:

\begin{itemize}
\tightlist
\item
  \textbf{Minimizing functions} of multiple real-valued variables
\item
  \textbf{Handling constraints} on variable values\\
\item
  \textbf{Emphasizing convex problems} with data science applications
\item
  \textbf{Connecting theory to practice} in machine learning,
  statistics, and data analysis
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, breakable, colframe=quarto-callout-important-color-frame, colback=white, rightrule=.15mm, bottomtitle=1mm, leftrule=.75mm, colbacktitle=quarto-callout-important-color!10!white, left=2mm, toptitle=1mm, titlerule=0mm, bottomrule=.15mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{\textbf{Core Focus}}, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, coltitle=black]

Our choice of topics is \textbf{motivated by relevance to data science}
--- the formulations and algorithms discussed are directly useful for
solving real-world problems in machine learning, statistics, and data
analysis.

\end{tcolorbox}

This chapter outlines several \textbf{key paradigms from data science}
and demonstrates how they can be formulated as continuous optimization
problems. Understanding the \textbf{smoothness properties and structure}
of these formulations is crucial for selecting appropriate algorithms.
\#\# Data Analysis and Optimization \{\#sec-data-analysis\}

The typical optimization problem in data analysis involves finding a
\textbf{model that balances} two competing objectives:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Agreement with collected data}
\item
  \textbf{Adherence to structural constraints} reflecting our beliefs
  about good models
\end{enumerate}

\subsection{The Data Science
Framework}\label{the-data-science-framework}

In a typical analysis problem, our \textbf{dataset} consists of \(m\)
objects:

\begin{equation}\phantomsection\label{eq-dataset}{D := \{(a_j, y_j), j = 1,2, \ldots, m\}}\end{equation}

where:

\textbf{Features (\(a_j\))} - Vector or matrix of features - Input
variables - Predictors

\textbf{Labels/Observations (\(y_j\))}\\
- Target values - Responses - Outcomes

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, breakable, colframe=quarto-callout-tip-color-frame, colback=white, rightrule=.15mm, bottomtitle=1mm, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, left=2mm, toptitle=1mm, titlerule=0mm, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{\textbf{Data Preprocessing}}, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, coltitle=black]

We assume the data has been \textbf{cleaned} so that all pairs
\((a_j, y_j)\) have consistent size and shape.

\end{tcolorbox}

\subsection{The Learning Objective}\label{the-learning-objective}

The \textbf{data analysis task} consists of discovering a function
\(\phi\) such that:

\[\phi(a_j) \approx y_j \quad \text{for most } j = 1,2, \ldots, m\]

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, breakable, colframe=quarto-callout-note-color-frame, colback=white, rightrule=.15mm, bottomtitle=1mm, leftrule=.75mm, colbacktitle=quarto-callout-note-color!10!white, left=2mm, toptitle=1mm, titlerule=0mm, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{\textbf{Terminology}}, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, coltitle=black]

The process of discovering the mapping \(\phi\) is often called
\textbf{``learning''} or \textbf{``training''}.

\end{tcolorbox}

The function œÜ is often deÔ¨Åned in terms of a vector or matrix of
parameters, which we denote in what follows by x or X (and occasionally
by other notation). With these parametrizations, the problem of
identifying œÜ becomes a traditional data-Ô¨Åtting problem: Find the
parameters x deÔ¨Åning œÜ such that œÜ(aj) ‚âàyj, j = 1,2, . . . ,m in some
optimal sense. Once we come up with a deÔ¨Ånition of the term ``optimal''
(and possibly also with restrictions on the values that we allow to
parameters to take), we have an optimization problem. Frequently, these
optimization formulations have objective functions of the
\textbf{finite-sum type}:

m m  j=1
\begin{equation}\phantomsection\label{eq-finite-sum-formulation}{L_D(x) := \frac{1}{m} \sum_{j=1}^{m} \ell(a_j, y_j; x)}\end{equation}
The function ‚Ñì(a,y;x) here represents a ``loss'' incurred for not
properly aligning our prediction \(\phi(a)\) with \(y\). Thus, the
objective \(L_D(x)\) measures the average loss accrued over the entire
data set when the parameter vector is equal to x. Once an appropriate
value of \(x\) (and thus \(\phi\)) has been learned from the data, we
can use it to make predictions about other items of data not in the set
\(D\) (Equation~\ref{eq-dataset}).

Given an unseen item of data \(\hat{a}\) of the same type as \(a_j\),
\(j = 1,2, \ldots, m\), we predict the label \(\hat{y}\) associated with
\(\hat{a}\) to be \(\phi(\hat{a})\).

The mapping \(\phi\) may also expose other structures and properties in
the data set:

\begin{itemize}
\tightlist
\item
  \textbf{Feature Selection}: Reveals that only a small fraction of the
  features in \(a_j\) are needed to reliably predict the label \(y_j\)
\item
  \textbf{Subspace Discovery}: When the parameter \(x\) is a matrix, it
  could reveal a low-dimensional subspace that contains most of the
  vectors \(a_j\)
\item
  \textbf{Structured Matrices}: Could reveal a matrix with particular
  structure (low-rank, sparse) such that observations of \(X\) prompted
  by the feature vectors \(a_j\) yield results close to \(y_j\) \#\#\#
  Types of Data Analysis Problems \{\#sec-problem-types\}
\end{itemize}

The form of the labels \(y_j\) differs according to the nature of the
data analysis problem:

\subsubsection{Regression}

If each \(y_j\) is a \textbf{real number}, we typically have a
\textbf{regression problem}.

\subsubsection{Classification}

When each \(y_j\) is a \textbf{label} (an integer from the set
\(\{1,2, \ldots, M\}\)) indicating that \(a_j\) belongs to one of \(M\)
classes:

\begin{itemize}
\tightlist
\item
  \textbf{Binary Classification}: \(M = 2\)
\item
  \textbf{Multiclass Classification}: \(M > 2\)
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, breakable, colframe=quarto-callout-note-color-frame, colback=white, rightrule=.15mm, bottomtitle=1mm, leftrule=.75mm, colbacktitle=quarto-callout-note-color!10!white, left=2mm, toptitle=1mm, titlerule=0mm, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, coltitle=black]

In data analysis problems arising in speech and image recognition, \(M\)
can be very large, of the order of thousands or more.

\end{tcolorbox}

\subsubsection{Unsupervised Learning}

The labels \(y_j\) may not even exist; the data set may contain only the
feature vectors \(a_j\), \(j = 1,2, \ldots, m\). There are still
interesting data analysis problems associated with these cases. For
example, we may wish to group

the \(a_j\) into \textbf{clusters} (where the vectors within each
cluster are deemed to be functionally similar) or identify a
\textbf{low-dimensional subspace} (or a collection of low-dimensional
subspaces) that approximately contains the \(a_j\).

In such problems, we are essentially learning the labels \(y_j\)
alongside the function \(\phi\). For example, in a clustering problem,
\(y_j\) could represent the cluster to which \(a_j\) is assigned.

\subsection{Data Complications and
Robustness}\label{data-complications-and-robustness}

Even after cleaning and preparation, the preceding setup may contain
many complications:

\begin{itemize}
\tightlist
\item
  \textbf{Noise and Corruption}: The quantities \((a_j,y_j)\) may
  contain noise or be otherwise corrupted, requiring the mapping
  \(\phi\) to be robust to such errors
\item
  \textbf{Missing Data}: Parts of the vectors \(a_j\) may be missing, or
  we may not know all the labels \(y_j\)
\item
  \textbf{Streaming Data}: The data may be arriving in streaming fashion
  rather than being available all at once, requiring \textbf{online
  learning} of \(\phi\) One consideration that arises frequently is that
  we wish to avoid overÔ¨Åtting the model to the data set D in (1.1). The
  particular data set D available to us can often be thought of as a
  Ô¨Ånite sample drawn from some underlying larger (perhaps inÔ¨Ånite)
  collection of possible data points, and we wish the function œÜ to
  perform well on the unobserved data points as well as the observed
  subset D. In other words, we want œÜ to be not too sensitive to the
  particular sample D that is used to deÔ¨Åne empirical objective
  functions such as (1.2). One way to avoid this issue is to modify the
  objective function by adding constraints or penalty terms, in a way
  that limits the ``complexity'' of the function œÜ. This process is
  typically called regularization. An optimization formulation that
  balances Ô¨Åt to the training data D, model complexity, and model
  structure is
  \begin{equation}\phantomsection\label{eq-regularized-optimization}{\min_{x \in \Omega} L_D(x) + \lambda \text{pen}(x)}\end{equation}
  where  is a set of allowable values for x, pen(¬∑) is a regularization
  function or regularizer, and Œª ‚â•0 is a regularization parameter. The
  regularizer usually takes lower values for parameters x that yield
  functions œÜ with lower complex- ity. (For example, œÜ may depend on
  fewer of the features in the data vectors aj or may be less
  oscillatory.) The parameter Œª can be ``tuned'' to provide an
  appropriate balance between Ô¨Åtting the data and lowering the
  complexity of œÜ: Smaller values of Œª tend to produce solutions that Ô¨Åt
  the training data D more accurately, while large values of Œª lead to
  less complex models.1 1 Interestingly, the concept of overÔ¨Åtting has
  been reexamined in recent years, particularly in the context of deep
  learning, where models that perfectly Ô¨Åt the training data are
  sometimes observed to also do a good job of classifying previously
  unseen data. This phenomenon is a topic of intense current research in
  the machine learning community. The constraint set  in (1.3) may be
  chosen to exclude values of x that are not relevant or useful in the
  context of the data analysis problem. For example, in some
  applications, we may not wish to consider values of x in which one or
  more components are negative, so we could set  to be the set of
  vectors whose components are all greater than or equal to zero. We now
  examine some particular problems in data science that give rise to
  formulations that are special cases of our master problem (1.3). We
  will see that a large variety of problems can be formulated using this
  general framework, but we will also see that within this framework,
  there is a wide range of structures that must be taken into account in
  choosing algorithms to solve these problems efÔ¨Åciently. \#\# Least
  Squares \{\#sec-least-squares\}
\end{itemize}

Probably the oldest and best-known data analysis problem is linear least
squares. Here, the data points \((a_j,y_j)\) lie in
\(\mathbb{R}^n \times \mathbb{R}\), and we solve:  j=1  aT j x ‚àíyj 2 = 1
\begin{equation}\phantomsection\label{eq-least-squares-complete}{\min_x \frac{1}{2m} \sum_{j=1}^{m} \|a_j^T x - y_j\|^2 = \frac{1}{2m} \|Ax - y\|_2^2}\end{equation}
where \(A\) is the matrix whose rows are \(a_j^T\),
\(j = 1,2, \ldots, m\) and \(y = (y_1,y_2, \ldots, y_m)^T\). In the
preceding terminology, the function \(\phi\) is defined by
\(\phi(a) := a^T x\). (We can introduce a nonzero intercept by adding an
extra parameter \(\beta \in \mathbb{R}\) and defining
\(\phi(a) := a^T x + \beta\).) This formulation can be motivated
statistically, as a maximum-likelihood estimate of x when the
observations yj are exact but for independent identically distributed
(i.i.d.) Gaussian noise. We can add a variety of penalty functions to
this basic least squares problem to impose desirable structure on x and,
hence, on œÜ. For example, ridge regression adds a squared ‚Ñì2-norm
penalty, resulting in
\[\min_x \frac{1}{2m} \|Ax - y\|_2^2 + \lambda \|x\|_2^2\] for some
parameter Œª \textgreater{} 0. The solution x of this regularized
formulation has less sensitivity to perturba- tions in the data (aj,yj).
The LASSO formulation
\begin{equation}\phantomsection\label{eq-lasso}{\min_x \frac{1}{2m} \|Ax - y\|_2^2 + \lambda \|x\|_1}\end{equation}
tends to yield solutions x that are sparse -- that is, containing
relatively few nonzero components (Tibshirani, 1996). This formulation
performs feature selection: The locations of the nonzero components in x
reveal those components of aj that are instrumental in determining the
observation yj. Besides its statistical appeal -- predictors that depend
on few features are potentially simpler and more comprehensible than
those depending on many features -- feature selection has practical
appeal in making predictions about future data. Rather than gathering
all components of a new data vector ÀÜa, we need to Ô¨Ånd only the
``selected'' features because only these are needed to make a
prediction. The LASSO formulation (1.5) is an important prototype for
many problems in data analysis in that it involves a regularization term
\(\lambda \|x\|_1\) that is non- smooth and convex but has relatively
simple structure that can potentially be exploited by algorithms. \#\#
Matrix Factorization Problems \{\#sec-matrix-factorization\}

There are a variety of data analysis problems that require estimating a
low-rank matrix from some sparse collection of data. Such problems can
be formulated as natural extension of least squares to problems in which
the data aj are naturally represented as matrices rather than vectors.
Changing notation slightly, we suppose that each Aj is an n√óp matrix,
and we seek another n √ó p matrix X that solves min X 1 2m m  j=1
(‚ü®Aj,X‚ü©‚àíyj)2, (1.6) where ‚ü®A,B‚ü©:= trace(AT B). Here we can think of the
Aj as ``probing'' the unknown matrix X. Commonly considered types of
observations are random linear combinations (where the elements of Aj
are selected i.i.d. from some distribution) or single-element
observations (in which each Aj has 1 in a single location and zeros
elsewhere). A regularized version of (1.6), leading to solutions X that
are low rank, is min X 1 2m m  j=1 (‚ü®Aj,X‚ü©‚àíyj)2 + Œª‚à•X‚à•‚àó, (1.7) where
\(\|X\|_*\) is the nuclear norm, which is the sum of singular values of
\(X\) (Recht et al., 2010). The nuclear norm plays a role analogous to
the ‚Ñì1 norm in (1.5), where as the ‚Ñì1 norm favors sparse vectors, the
nuclear norm favors low- rank matrices. Although the nuclear norm is a
somewhat complex nonsmooth function, it is at least convex so that the
formulation (1.7) is also convex. This formulation can be shown to yield
a statistically valid solution when the true
https://doi.org/10.1017/9781009004282.002 Published online by Cambridge
University Press 6 1 Introduction X is low rank and the observation
matrices Aj satisfy a ``restricted isometry property,'' commonly
satisÔ¨Åed by random matrices but not by matrices with just one nonzero
element. The formulation is also valid in a different context, in which
the true X is incoherent (roughly speaking, it does not have a few
elements that are much larger than the others), and the observations Aj
are of single elements (Cand`es and Recht, 2009). In another form of
regularization, the matrix X is represented explicitly as a product of
two ``thin'' matrices L and R, where L ‚ààRn√ór and R ‚ààRp√ór, with r
‚â™min(n,p). We set X = LRT in (1.6) and solve min L,R 1 2m m  j=1
(‚ü®Aj,LRT ‚ü©‚àíyj)2. (1.8) In this formulation, the rank r is ``hard-wired''
into the deÔ¨Ånition of X, so there is no need to include a regularizing
term. This formulation is also typically much more compact than (1.7);
the total number of elements in (L,R) is (n + p)r, which is much less
than np. However, this function is nonconvex when considered as a
function of (L,R) jointly. An active line of current research, pioneered
by Burer and Monteiro (2003) and also drawing on statistical sources,
shows that the nonconvexity is benign in many situations and that, under
certain assumptions on the data (Aj,yj), j = 1,2, . . . ,m and careful
choice of algorithmic strategy, good solutions can be obtained from the
formulation (1.8). A clue to this good behavior is that although this
formulation is nonconvex, it is in some sense an approximation to a
tractable problem: If we have a complete observation of X, then a rank-r
approximation can be found by performing a singular value decomposition
of X and deÔ¨Åning L and R in terms of the r leading left and right
singular vectors. Some applications in computer vision, chemometrics,
and document clus- tering require us to Ô¨Ånd factors L and R like those
in (1.8) in which all elements are nonnegative. If the full matrix Y
‚ààRn√óp is observed, this problem has the form
\[\min_{L,R} \|LR^T - Y\|_F^2 \quad \text{subject to } L \geq 0, R \geq 0\]
and is called nonnegative matrix factorization. \#\# Support Vector
Machines \{\#sec-svm\}

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, breakable, colframe=quarto-callout-note-color-frame, colback=white, rightrule=.15mm, bottomtitle=1mm, leftrule=.75mm, colbacktitle=quarto-callout-note-color!10!white, left=2mm, toptitle=1mm, titlerule=0mm, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{\textbf{Classical ML Problem}}, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, coltitle=black]

\textbf{Classification via Support Vector Machines (SVM)} is a classical
optimization problem in machine learning, tracing its origins to the
\textbf{1960s}.

\end{tcolorbox}

Given the input data (aj,yj) with aj ‚ààRn and yj ‚àà\{‚àí1,1\}, SVM seeks a
vector x ‚ààRn and a scalar Œ≤ ‚ààR such that
\begin{equation}\phantomsection\label{eq-svm-positive}{a_j^T x - \beta \geq 1 \quad \text{when } y_j = +1}\end{equation}
\begin{equation}\phantomsection\label{eq-svm-negative}{a_j^T x - \beta \leq -1 \quad \text{when } y_j = -1}\end{equation}
Any pair (x,Œ≤) that satisÔ¨Åes these conditions deÔ¨Ånes a separating
hyperplane in Rn, that separates the ``positive'' cases \{aj \textbar{}
yj = +1\} from the ``negative'' cases \{aj \textbar{} yj = ‚àí1\}. Among
all separating hyperplanes, the one that minimizes \(\|x\|_2\) is the
one that maximizes the margin between the two classes -- that is, the
hyperplane whose distance to the nearest point aj of either class is
greatest. We can formulate the problem of Ô¨Ånding a separating hyperplane
as an optimization problem by deÔ¨Åning an objective with the summation
form (1.2): H(x,Œ≤) = 1 m m  j=1 max(1 ‚àíyj(aT j x ‚àíŒ≤),0). (1.10) Note
that the jth term in this summation is zero if the conditions (1.9) are
satisÔ¨Åed, and it is positive otherwise. Even if no pair (x,Œ≤) exists for
which H(x,Œ≤) = 0, a value (x,Œ≤) that minimizes (1.2) will be the one
that comes as close as possible to satisfying (1.9) in some sense. A
term \(\lambda \|x\|_2^2\) 2 (for some parameter Œª \textgreater{} 0) is
often added to (1.10), yielding the following regularized version:
H(x,Œ≤) = 1 m m  j=1 max(1 ‚àíyj(aT j x ‚àíŒ≤),0) + 1 2Œª‚à•x‚à•2 2. (1.11) Note
that, in contrast to the examples presented so far, the SVM problem has
a nonsmooth loss function and a smooth regularizer. If Œª is sufÔ¨Åciently
small, and if separating hyperplanes exist, the pair (x,Œ≤) that
minimizes (1.11) is the maximum-margin separating hyperplane. The
maximum-margin property is consistent with the goals of generalizability
and robustness. For example, if the observed data (aj,yj) is drawn from
an underlying ``cloud'' of positive and negative cases, the
maximum-margin solution usually does a reasonable job of separating
other empirical data samples drawn from the same clouds, whereas a
hyperplane that passes close to several of the observed data points may
not do as well (see Figure 1.1). Often, it is not possible to Ô¨Ånd a
hyperplane that separates the positive and negative cases well enough to
be useful as a classiÔ¨Åer. One solution is to transform all of the raw
data vectors aj by some nonlinear mapping œà and . Figure 1.1 Linear
support vector machine classiÔ¨Åcation, with the one class represented by
circles and the other by squares. One possible choice of separating
hyperplane is shown at left. If the training data is an empirical sample
drawn from a cloud of underlying data points, this plane does not do
well in separating the two clouds (middle). The maximum-margin
separating hyperplane does better (right). then perform the support
vector machine classiÔ¨Åcation on the vectors œà(aj), j = 1,2, . . . ,m.
The conditions (1.9) would thus be replaced by
\begin{equation}\phantomsection\label{eq-svm-kernel-positive}{\psi(a_j)^T x - \beta \geq 1 \quad \text{when } y_j = +1}\end{equation}
\begin{equation}\phantomsection\label{eq-svm-kernel-negative}{\psi(a_j)^T x - \beta \leq -1 \quad \text{when } y_j = -1}\end{equation}
leading to the following analog of (1.11): H(x,Œ≤) = 1 m m  j=1 max(1
‚àíyj(œà(aj)\^{}T x ‚àíŒ≤),0) + 1 2Œª‚à•x‚à•2 2. (1.13) When transformed back to
Rm, the surface \{a \textbar{} œà(a)\^{}T x ‚àíŒ≤ = 0\} is nonlinear and
possibly disconnected, and is often a much more powerful classiÔ¨Åer than
the hyperplanes resulting from (1.11). We note that SVM can also be
expressed naturally as a minimization problem over a convex set. By
introducing artiÔ¨Åcial variables, the problem (1.13) (and (1.11)) can be
formulated as a convex quadratic program -- that is, a problem with a
convex quadratic objective and linear constraints. By taking the dual of
this problem, we obtain another convex quadratic program, in m
variables:
\begin{equation}\phantomsection\label{eq-svm-dual}{\begin{aligned}
\min_{\alpha \in \mathbb{R}^m} \quad & \frac{1}{2}\alpha^T Q\alpha - \mathbf{1}^T \alpha \\
\text{subject to} \quad & 0 \leq \alpha \leq \frac{1}{\lambda}\mathbf{1}, \quad y^T \alpha = 0
\end{aligned}}\end{equation} where: -
\(Q_{kl} = y_k y_l \psi(a_k)^T \psi(a_l)\) -
\(y = (y_1, y_2, \ldots, y_m)^T\)\\
- \(\mathbf{1} = (1, 1, \ldots, 1)^T\) Interestingly, problem (1.14) can
be formulated and solved without explicit knowledge or deÔ¨Ånition of the
mapping œà. We need only a technique to deÔ¨Åne the elements of Q. This can
be done with the use of a kernel function K : Rn √ó Rn ‚ÜíR, where
\(K(a_k,a_l)\) replaces \(\psi(a_k)^T \psi(a_l)\) (Boser et al., 1992;
Cortes https://doi.org/10.1017/9781009004282.002 Published online by
Cambridge University Press

and Vapnik, 1995). This is the so-called kernel trick. (The kernel
function K can also be used to construct a classiÔ¨Åcation function œÜ from
the solution of (1.14).) A particularly popular choice of kernel is the
Gaussian kernel:
\[K(a_k,a_l) := \exp\left(-\frac{1}{2\sigma^2} \|a_k - a_l\|_2^2\right)\]


where \(\sigma\) is a positive parameter.

\section{Logistic Regression}\label{sec-logistic-regression}

Logistic regression can be viewed as a softened form of binary support
vector machine classiÔ¨Åcation in which, rather than the classiÔ¨Åcation
function œÜ giving a unqualiÔ¨Åed prediction of the class in which a new
data vector a lies, it returns an estimate of the odds of a belonging to
one class or the other. We seek an ``odds function'' p parametrized by a
vector x ‚ààRn, \[p(a;x) := (1 + \exp(a^T x))^{-1}\] (1.15) and aim to
choose the parameter x in so that - \(p(a_j;x) \approx 1\) when
\(y_j = +1\) - \(p(a_j;x) \approx 0\) when \(y_j = -1\) (Note the
similarity to (1.9).) The optimal value of x can be found by minimizing
a negative-log-likelihood function: L(x) := ‚àí1 m ‚é° ‚é£ j:yj =‚àí1 log(1
‚àíp(aj;x)) +  j:yj =1 log p(aj;x) ‚é§ ‚é¶. (1.17) Note that the deÔ¨Ånition
(1.15) ensures that p(a;x) ‚àà(0,1) for all a and x; thus, log(1 ‚àíp(aj;x))
\textless{} 0 and log p(aj;x) \textless{} 0 for all j and all x. When
the conditions (1.16) are satisÔ¨Åed, these log terms will be only
slightly negative, so values of x that satisfy (1.17) will be near
optimal. We can perform feature selection using the model (1.17) by
introducing a regularizer \(\lambda \|x\|_1\) (as in the LASSO technique
for least squares), min x ‚àí1 m ‚é° ‚é£ j:yj =‚àí1 log(1 ‚àíp(aj;x)) +  j:yj =1
log p(aj;x) ‚é§ ‚é¶+ Œª‚à•x‚à•1, (1.18) where Œª \textgreater{} 0 is a
regularization parameter. As we see later, this term has the effect of
producing a solution in which few components of x are nonzero,
https://doi.org/10.1017/9781009004282.002 Published online by Cambridge
University Press 10 1 Introduction making it possible to evaluate p(a;x)
by knowing only those components of a that correspond to the nonzeros in
x. An important extension of this technique is to multiclass (or
multinomial) logistic regression, in which the data vectors aj belong to
more than two classes. Such applications are common in modern data
analysis. For example, in a speech recognition system, the M classes
could each represent a phoneme of speech, one of the potentially
thousands of distinct elementary sounds that can be uttered by humans in
a few tens of milliseconds. A multinomial logistic regression problem
requires a distinct odds function pk for each class k ‚àà\{1,2, . . .
,M\}. These functions are parametrized by vectors x{[}k{]} ‚ààRn, k = 1,2,
. . . ,M, deÔ¨Åned as follows: pk(a;X) := \exp(a\^{}T x\^{}\{{[}k{]}\}) M
l=1 \exp(a\^{}T x\^{}\{{[}l{]}\}) , k = 1,2, . . . ,M, (1.19) where we
deÔ¨Åne X := \{x{[}k{]} \textbar{} k = 1,2, . . . ,M\}. As in the binary
case, we have pk(a) ‚àà(0,1) for all a and all k = 1,2, . . . ,M and, in
addition, that M k=1 pk(a) = 1. The functions (1.19) perform a
``softmax'' on the quantities \{aT x{[}l{]} \textbar{} l = 1,2, . . .
,M\}. In the setting of multiclass logistic regression, the labels yj
are vectors in RM whose elements are deÔ¨Åned as follows: yjk =  1 when aj
belongs to class k, 0 otherwise. (1.20) Similarly to (1.16), we seek to
deÔ¨Åne the vectors x{[}k{]} so that - \(p_k(a_j;X) \approx 1\) when
\(y_{jk} = 1\) - \(p_k(a_j;X) \approx 0\) when \(y_{jk} = 0\) The
problem of Ô¨Ånding values of x{[}k{]} that satisfy these conditions can
again be formulated as one of minimizing a negative-log-likelihood: L(X)
:= ‚àí1 m m  j=1

M  ‚Ñì=1 y\_\{j\ell\}(x\^{}\{{[}\ell{]}T\}a\_j) ‚àílog  M  ‚Ñì=1
\exp(x\^{}\{{[}\ell{]}T\}a\_j)  . (1.22) ``Group-sparse'' regularization
terms can be included in this formulation to select a set of features in
the vectors aj, common to each class, that distinguish effectively
between the classes. https://doi.org/10.1017/9781009004282.002 Published
online by Cambridge University Press 1.6 Deep Learning \#\# Deep
Learning \{\#sec-deep-learning\}

Deep neural networks are often designed to perform the same function as
multiclass logistic regression -- that is, to classify a data vector a
into one of M possible classes, often for large M. The major innovation
is that the mapping œÜ from data vector to prediction is now a nonlinear
function, explicitly parametrized by a set of structured
transformations. The neural network shown in Figure 1.2 illustrates the
structure of a particu- lar neural net. In this Ô¨Ågure, the data vector
aj enters at the left of the network, and each box (more often referred
to as a ``layer'') represents a transformation that takes an input
vector and applies a nonlinear transformation of the data to produce an
output vector. The output of each operator becomes the input for one or
more subsequent layers. Each layer has a set of its own parameters, and
the collection of all of the parameters over all the layers comprises
our optimization variable. The different shades of boxes here denote the
fact that the types of transformations might differ between layers, but
we can compose them in whatever fashion suits our application. A typical
transformation, which converts the vector al‚àí1 j representing output
from layer l ‚àí1 to the vector al j representing output from layer l, is
al j = œÉ(W lal‚àí1 j + gl), (1.23) where W l is a matrix of dimension
\textbar al j\textbar√ó\textbar al‚àí1 j \textbar{} and gl is a vector of
length \textbar al j\textbar. The function œÉ is a componentwise
nonlinear transformation, usually called an activation function. The
most common forms of the activation function œÉ act independently on each
component of their argument vector as follows: - Sigmoid: t ‚Üí1/(1 +
e‚àít); - RectiÔ¨Åed Linear Unit (ReLU): t ‚Üímax(t,0). Alternative
transformations are needed when the input to box l comes from two or
more preceding boxes (as in the case for some boxes in Figure 1.2). The
rightmost layer of the neural network (the output layer) typically has M
outputs, one for each of the possible classes to which the input (aj,
say) could belong. These are compared to the labels yjk, deÔ¨Åned as in
(1.20) to indicate which of the M classes that aj belongs to. Often, a
softmax is applied to the Figure 1.2 A deep neural network, showing
connections between adjacent layers, where each layer is represented by
a shaded rectangle. https://doi.org/10.1017/9781009004282.002 Published
online by Cambridge University Press 12 1 Introduction outputs in the
rightmost layer, and a loss function similar to (1.22) is obtained, as
we describe now. Consider the special (but not uncommon) case in which
the neural net structure is a linear graph of D levels, in which the
output for layer l ‚àí1 becomes the input for layer l (for l = 1,2, . . .
,D) with aj = a0 j, j = 1,2, . . . ,m, and the transformation within
each box has the form (1.23). A softmax is applied to the output of the
rightmost layer to obtain a set of odds. The parameters in this neural
network are the matrix-vector pairs (W l,gl), l = 1,2, . . . ,D that
transform the input vector aj = a0 j into the output aD j of the Ô¨Ånal
layer. We aim to choose all these parameters so that the network does a
good job of classifying the training data correctly. Using the notation
w for the layer-to-layer transformations, that is, w := (W 1,g1,W 2,g2,
. . . ,W D,gD), we can write the loss function for deep learning as L(w)
= ‚àí1 m m  j=1

M  ‚Ñì=1 y\_\{j\ell\}a\^{}D\_\{j,\ell\}(w) ‚àílog  M  ‚Ñì=1 exp aD j,‚Ñì(w)  ,
(1.24) where aD j,‚Ñì(w) ‚ààR is the output of the ‚Ñìth element in layer D
corresponding to input vector a0 j. (Here we write aD j,‚Ñì(w) to make
explicit the dependence on the transformations w as well as on the input
vector aj.) We can view multiclass logistic regression as a special case
of deep learning with D = 1, so that a1 j,‚Ñì= W 1 ‚Ñì,¬∑a0 j, where W 1 ‚Ñì,¬∑
denotes row ‚Ñìof the matrix W 1. Neural networks in use for particular
applications (for example, in image recognition and speech recognition,
where they have been quite successful) include many variants on the
basic design. These include restricted connectiv- ity between the boxes
(which corresponds to enforcing sparsity structure on the matrices W l,
l = 1,2, . . . ,D) and sharing parameters, which corresponds to forcing
subsets of the elements of W l to take the same value. Arrangements of
the boxes may be quite complex, with outputs coming from several layers,
con- nections across nonadjacent layers, different componentwise
transformations œÉ at different layers, and so on. Deep neural networks
for practical applications are highly engineered objects. The loss
function (1.24) shares with many other applications the Ô¨Ånite-sum form
(1.2), but it has several features that set it apart from the other
applications discussed before. First, and possibly most important, it is
nonconvex in the parameters w. Second, the total number of parameters in
w is usually very large. Effective training of deep learning classiÔ¨Åers
typically requires a great deal of data and computation power. Huge
clusters of powerful computers --
https://doi.org/10.1017/9781009004282.002 Published online by Cambridge
University Press 1.7 Emphasis 13 often using multicore processors, GPUs,
and even specially architected pro- cessing units -- are devoted to this
task. \#\# Emphasis \{\#sec-emphasis\}

Many problems can be formulated as in the framework (1.3), and their
properties may differ signiÔ¨Åcantly. They might be convex or nonconvex,
and smooth or nonsmooth. But there are important features that they all
share. ‚Ä¢ They can be formulated as functions of real variables, which we
typically arrange in a vector of length n. ‚Ä¢ The functions are
continuous. When nonsmoothness appears in the formulation, it does so in
a structured way that can be exploited by the algorithm. Smoothness
properties allow an algorithm to make good inferences about the behavior
of the function on the basis of knowledge gained at nearby points that
have been visited previously. ‚Ä¢ The objective is often made up in part
of a summation of many terms, where each term depends on a single item
of data. ‚Ä¢ The objective is often a sum of two terms: a ``loss term''
(sometimes arising from a maximum likelihood expression for some
statistical model) and a ``regularization term'' whose purpose is to
impose structure and ``generalizability'' on the recovered model. Our
treatment emphasizes algorithms for solving these various kinds of
problems, with analysis of the convergence properties of these
algorithms. We pay attention to complexity guarantees, which are bounds
on the amount of computational effort required to obtain solutions of a
given accuracy. These bounds usually depend on fundamental properties of
the objective function and the data that deÔ¨Ånes it, including the
dimensions of the data set and the number of variables in the problem.
This emphasis contrasts with much of the optimization literature, in
which global convergence results do not usually involve complexity
bounds. (A notable exception is the analysis of interior- point methods
(see Nesterov and Nemirovskii, 1994; Wright, 1997)). At the same time,
we try as much as possible to emphasize the practical concerns
associated with solving these problems. There are a variety of trade-
offs presented by any problem, and the optimizer has to evaluate which
tools are most appropriate to use. On top of the problem formulation, it
is imperative to account for the time budget for the task at hand, the
type of computer on which the problem will be solved, and the guarantees
needed for the https://doi.org/10.1017/9781009004282.002 Published
online by Cambridge University Press

\part{Summary \& References}

\chapter{Summary}\label{summary}

In summary, this book has no content whatsoever.

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-knuth84}
Knuth, Donald E. 1984. {``Literate Programming.''} \emph{Comput. J.} 27
(2): 97--111. \url{https://doi.org/10.1093/comjnl/27.2.97}.

\end{CSLReferences}




\end{document}
