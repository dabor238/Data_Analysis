[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis Collection",
    "section": "",
    "text": "Choose your language / Elige tu idioma:\n\n\n\nComprehensive collection of data analysis methods, optimization techniques, and machine learning fundamentals.\nTopics: - Optimization Methods - Statistical Analysis - Machine Learning\nGo to English Book →\n\n\n\nColección completa de métodos de análisis de datos, técnicas de optimización y fundamentos de aprendizaje automático.\nTemas: - Métodos de Optimización - Análisis Estadístico - Aprendizaje Automático\nIr al Libro en Español →"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/optimization/01_intro.html",
    "href": "chapters/optimization/01_intro.html",
    "title": "2  Introduction to Optimization for Data Science",
    "section": "",
    "text": "2.1 Introduction\nThis book focuses on the fundamentals of algorithms for solving continuous optimization problems, which involve:\nThis chapter outlines several key paradigms from data science and demonstrates how they can be formulated as continuous optimization problems. Understanding the smoothness properties and structure of these formulations is crucial for selecting appropriate algorithms.",
    "crumbs": [
      "Optimization Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Optimization for Data Science</span>"
    ]
  },
  {
    "objectID": "chapters/optimization/01_intro.html#sec-introduction",
    "href": "chapters/optimization/01_intro.html#sec-introduction",
    "title": "2  Introduction to Optimization for Data Science",
    "section": "",
    "text": "Minimizing functions of multiple real-valued variables\nHandling constraints on variable values\n\nEmphasizing convex problems with data science applications\nConnecting theory to practice in machine learning, statistics, and data analysis\n\n\n\n\n\n\n\nImportantCore Focus\n\n\n\nOur choice of topics is motivated by relevance to data science — the formulations and algorithms discussed are directly useful for solving real-world problems in machine learning, statistics, and data analysis.",
    "crumbs": [
      "Optimization Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Optimization for Data Science</span>"
    ]
  },
  {
    "objectID": "chapters/optimization/01_intro.html#sec-data-analysis",
    "href": "chapters/optimization/01_intro.html#sec-data-analysis",
    "title": "2  Introduction to Optimization for Data Science",
    "section": "2.2 Data Analysis and Optimization",
    "text": "2.2 Data Analysis and Optimization\nThe typical optimization problem in data analysis involves finding a model that balances two competing objectives:\n\nAgreement with collected data\nAdherence to structural constraints reflecting our beliefs about good models\n\n\n2.2.1 The Data Science Framework\nIn a typical analysis problem, our dataset consists of \\(m\\) objects:\n\\[D := \\{(a_j, y_j), j = 1,2, \\ldots, m\\} \\tag{2.1}\\]\nwhere:\n\n\nFeatures (\\(a_j\\))\n\nVector or matrix of features\nInput variables\nPredictors\n\n\n\nLabels/Observations (\\(y_j\\))\n\nTarget values\nResponses\nOutcomes\n\n\n\n\n\n\n\n\n\nTipData Preprocessing\n\n\n\nWe assume the data has been cleaned so that all pairs \\((a_j, y_j)\\) have consistent size and shape.\n\n\n\n\n2.2.2 The Learning Objective\nThe data analysis task consists of discovering a function \\(\\phi\\) such that:\n\\[\\phi(a_j) \\approx y_j \\quad \\text{for most } j = 1,2, \\ldots, m\\]\n\n\n\n\n\n\nNoteTerminology\n\n\n\nThe process of discovering the mapping \\(\\phi\\) is often called “learning” or “training”.\n\n\n\n\n2.2.3 Parametrization and Optimization Formulation\nThe function \\(\\phi\\) is often defined in terms of a vector or matrix of parameters, which we denote by \\(x\\) or \\(X\\). With these parametrizations, the problem of identifying \\(\\phi\\) becomes a traditional data-fitting problem:\n\n\n\n\n\n\nTipOptimization Goal\n\n\n\nFind the parameters \\(x\\) defining \\(\\phi\\) such that \\(\\phi(a_j) \\approx y_j\\) for \\(j = 1,2, \\ldots, m\\) in some optimal sense.\n\n\nOnce we define “optimal” (and possibly add restrictions on allowable parameter values), we have an optimization problem.\n\n\n2.2.4 Finite-Sum Formulation\nFrequently, these optimization formulations have objective functions of the finite-sum type:\n\\[L_D(x) := \\frac{1}{m} \\sum_{j=1}^{m} \\ell(a_j, y_j; x) \\tag{2.2}\\]\nwhere:\n\n\\(\\ell(a,y;x)\\) represents the “loss” incurred for not properly aligning our prediction \\(\\phi(a)\\) with \\(y\\)\n\\(L_D(x)\\) measures the average loss accrued over the entire data set when the parameter vector equals \\(x\\)\n\n\n\n2.2.5 Prediction and Model Properties\nOnce an appropriate value of \\(x\\) (and thus \\(\\phi\\)) has been learned from the data, we can use it to make predictions about other items of data not in the set \\(D\\) (Equation 2.1).\nGiven an unseen item of data \\(\\hat{a}\\) of the same type as \\(a_j\\), \\(j = 1,2, \\ldots, m\\), we predict the label \\(\\hat{y}\\) associated with \\(\\hat{a}\\) to be \\(\\phi(\\hat{a})\\).\nThe mapping \\(\\phi\\) may also expose other structures and properties in the data set:\n\nFeature Selection: Reveals that only a small fraction of the features in \\(a_j\\) are needed to reliably predict the label \\(y_j\\)\nSubspace Discovery: When the parameter \\(x\\) is a matrix, it could reveal a low-dimensional subspace that contains most of the vectors \\(a_j\\)\nStructured Matrices: Could reveal a matrix with particular structure (low-rank, sparse) such that observations of \\(X\\) prompted by the feature vectors \\(a_j\\) yield results close to \\(y_j\\)\n\n\n\n2.2.6 Types of Data Analysis Problems\nThe form of the labels \\(y_j\\) differs according to the nature of the data analysis problem:\n\nRegressionClassificationUnsupervised Learning\n\n\nIf each \\(y_j\\) is a real number, we typically have a regression problem.\n\n\nWhen each \\(y_j\\) is a label (an integer from the set \\(\\{1,2, \\ldots, M\\}\\)) indicating that \\(a_j\\) belongs to one of \\(M\\) classes:\n\nBinary Classification: \\(M = 2\\)\nMulticlass Classification: \\(M &gt; 2\\)\n\n\n\n\n\n\n\nNote\n\n\n\nIn data analysis problems arising in speech and image recognition, \\(M\\) can be very large, of the order of thousands or more.\n\n\n\n\nThe labels \\(y_j\\) may not even exist; the data set may contain only the feature vectors \\(a_j\\), \\(j = 1,2, \\ldots, m\\). There are still interesting data analysis problems associated with these cases. For example, we may wish to group the \\(a_j\\) into clusters (where the vectors within each cluster are deemed to be functionally similar) or identify a low-dimensional subspace (or a collection of low-dimensional subspaces) that approximately contains the \\(a_j\\).\nIn such problems, we are essentially learning the labels \\(y_j\\) alongside the function \\(\\phi\\). For example, in a clustering problem, \\(y_j\\) could represent the cluster to which \\(a_j\\) is assigned.\n\n\n\n\n\n2.2.7 Data Complications and Robustness\nEven after cleaning and preparation, the preceding setup may contain many complications:\n\nNoise and Corruption: The quantities \\((a_j,y_j)\\) may contain noise or be otherwise corrupted, requiring the mapping \\(\\phi\\) to be robust to such errors\nMissing Data: Parts of the vectors \\(a_j\\) may be missing, or we may not know all the labels \\(y_j\\)\nStreaming Data: The data may be arriving in streaming fashion rather than being available all at once, requiring online learning of \\(\\phi\\)\n\n\n\n2.2.8 Regularization and Overfitting\nOne consideration that arises frequently is that we wish to avoid overfitting the model to the data set \\(D\\) in (Equation 2.1).\n\n\n\n\n\n\nImportantGeneralization Goal\n\n\n\nThe particular data set \\(D\\) available to us can often be thought of as a finite sample drawn from some underlying larger (perhaps infinite) collection of possible data points. We wish the function \\(\\phi\\) to perform well on the unobserved data points as well as the observed subset \\(D\\).\n\n\nIn other words, we want \\(\\phi\\) to be not too sensitive to the particular sample \\(D\\) that is used to define empirical objective functions such as (Equation 2.2).\nOne way to avoid this issue is to modify the objective function by adding constraints or penalty terms, in a way that limits the “complexity” of the function \\(\\phi\\). This process is typically called regularization.\nAn optimization formulation that balances fit to the training data \\(D\\), model complexity, and model structure is:\n\\[\\min_{x \\in \\Omega} L_D(x) + \\lambda \\text{pen}(x) \\tag{2.3}\\]\nwhere:\n\n\\(\\Omega\\) is a set of allowable values for \\(x\\)\n\\(\\text{pen}(\\cdot)\\) is a regularization function or regularizer\n\\(\\lambda \\geq 0\\) is a regularization parameter\n\nThe regularizer usually takes lower values for parameters \\(x\\) that yield functions \\(\\phi\\) with lower complexity. For example, \\(\\phi\\) may depend on fewer of the features in the data vectors \\(a_j\\) or may be less oscillatory.\n\n2.2.8.1 Tuning the Regularization Parameter\nThe parameter \\(\\lambda\\) can be “tuned” to provide an appropriate balance:\n\nSmaller values of \\(\\lambda\\): Produce solutions that fit the training data \\(D\\) more accurately\nLarger values of \\(\\lambda\\): Lead to less complex models\n\n\n\n\n\n\n\nNoteModern Perspective on Overfitting\n\n\n\n\n\nInterestingly, the concept of overfitting has been reexamined in recent years, particularly in the context of deep learning, where models that perfectly fit the training data are sometimes observed to also do a good job of classifying previously unseen data. This phenomenon is a topic of intense current research in the machine learning community.\n\n\n\n\n\n2.2.8.2 Constraint Sets\nThe constraint set \\(\\Omega\\) in (Equation 2.3) may be chosen to exclude values of \\(x\\) that are not relevant or useful in the context of the data analysis problem. For example:\n\nIn some applications, we may not wish to consider values of \\(x\\) in which one or more components are negative\nWe could set \\(\\Omega\\) to be the set of vectors whose components are all greater than or equal to zero\n\n\n\n\n2.2.9 Framework Overview\nWe now examine some particular problems in data science that give rise to formulations that are special cases of our master problem (Equation 2.3). We will see that:\n\nA large variety of problems can be formulated using this general framework\nWithin this framework, there is a wide range of structures that must be taken into account in choosing algorithms to solve these problems efficiently",
    "crumbs": [
      "Optimization Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Optimization for Data Science</span>"
    ]
  },
  {
    "objectID": "chapters/optimization/01_intro.html#sec-least-squares",
    "href": "chapters/optimization/01_intro.html#sec-least-squares",
    "title": "2  Introduction to Optimization for Data Science",
    "section": "2.3 Least Squares",
    "text": "2.3 Least Squares\nProbably the oldest and best-known data analysis problem is linear least squares. Here, the data points \\((a_j,y_j)\\) lie in \\(\\mathbb{R}^n \\times \\mathbb{R}\\), and we solve:\n\\[\\min_x \\frac{1}{2m} \\sum_{j=1}^{m} (a_j^T x - y_j)^2 = \\frac{1}{2m} \\|Ax - y\\|_2^2 \\tag{2.4}\\]\nwhere:\n\n\\(A\\) is the matrix whose rows are \\(a_j^T\\), \\(j = 1,2, \\ldots, m\\)\n\\(y = (y_1,y_2, \\ldots, y_m)^T\\)\n\nIn the preceding terminology, the function \\(\\phi\\) is defined by \\(\\phi(a) := a^T x\\).\n\n\n\n\n\n\nTipAdding an Intercept\n\n\n\nWe can introduce a nonzero intercept by adding an extra parameter \\(\\beta \\in \\mathbb{R}\\) and defining \\(\\phi(a) := a^T x + \\beta\\).\n\n\n\n2.3.1 Statistical Motivation\nThis formulation can be motivated statistically, as a maximum-likelihood estimate of \\(x\\) when the observations \\(y_j\\) are exact but for independent identically distributed (i.i.d.) Gaussian noise.\n\n\n2.3.2 Ridge Regression\nWe can add a variety of penalty functions to this basic least squares problem to impose desirable structure on \\(x\\) and, hence, on \\(\\phi\\). For example, ridge regression adds a squared \\(\\ell_2\\)-norm penalty:\n\\[\\min_x \\frac{1}{2m} \\|Ax - y\\|_2^2 + \\lambda \\|x\\|_2^2 \\tag{2.5}\\]\nfor some parameter \\(\\lambda &gt; 0\\). The solution \\(x\\) of this regularized formulation has less sensitivity to perturbations in the data \\((a_j,y_j)\\).\n\n\n2.3.3 LASSO Formulation\nThe LASSO (Least Absolute Shrinkage and Selection Operator) formulation:\n\\[\\min_x \\frac{1}{2m} \\|Ax - y\\|_2^2 + \\lambda \\|x\\|_1 \\tag{2.6}\\]\ntends to yield solutions \\(x\\) that are sparse – that is, containing relatively few nonzero components.\n\n\n\n\n\n\nImportantFeature Selection\n\n\n\nThis formulation performs feature selection: The locations of the nonzero components in \\(x\\) reveal those components of \\(a_j\\) that are instrumental in determining the observation \\(y_j\\).\n\n\n\n2.3.3.1 Advantages of Feature Selection\n\nStatistical Appeal: Predictors that depend on few features are potentially simpler and more comprehensible than those depending on many features\nPractical Benefits: Rather than gathering all components of a new data vector \\(\\hat{a}\\), we need to find only the “selected” features because only these are needed to make a prediction\n\n\n\n\n\n\n\nNoteLASSO as a Prototype\n\n\n\nThe LASSO formulation (Equation 2.6) is an important prototype for many problems in data analysis in that it involves a regularization term \\(\\lambda \\|x\\|_1\\) that is nonsmooth and convex but has relatively simple structure that can potentially be exploited by algorithms.",
    "crumbs": [
      "Optimization Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Optimization for Data Science</span>"
    ]
  },
  {
    "objectID": "chapters/optimization/01_intro.html#sec-matrix-factorization",
    "href": "chapters/optimization/01_intro.html#sec-matrix-factorization",
    "title": "2  Introduction to Optimization for Data Science",
    "section": "2.4 Matrix Factorization Problems",
    "text": "2.4 Matrix Factorization Problems\nThere are a variety of data analysis problems that require estimating a low-rank matrix from some sparse collection of data. Such problems can be formulated as natural extension of least squares to problems in which the data \\(a_j\\) are naturally represented as matrices rather than vectors.\n\n2.4.1 Basic Matrix Sensing Problem\nChanging notation slightly, we suppose that each \\(A_j\\) is an \\(n \\times p\\) matrix, and we seek another \\(n \\times p\\) matrix \\(X\\) that solves:\n\\[\\min_X \\frac{1}{2m} \\sum_{j=1}^{m} (\\langle A_j, X \\rangle - y_j)^2 \\tag{2.7}\\]\nwhere \\(\\langle A, B \\rangle := \\text{trace}(A^T B)\\).\nHere we can think of the \\(A_j\\) as “probing” the unknown matrix \\(X\\). Commonly considered types of observations are:\n\nRandom linear combinations: Elements of \\(A_j\\) are selected i.i.d. from some distribution\nSingle-element observations: Each \\(A_j\\) has 1 in a single location and zeros elsewhere\n\n\n\n2.4.2 Nuclear Norm Regularization\nA regularized version of (Equation 2.7), leading to solutions \\(X\\) that are low rank, is:\n\\[\\min_X \\frac{1}{2m} \\sum_{j=1}^{m} (\\langle A_j, X \\rangle - y_j)^2 + \\lambda \\|X\\|_* \\tag{2.8}\\]\nwhere \\(\\|X\\|_*\\) is the nuclear norm, which is the sum of singular values of \\(X\\).\n\n\n\n\n\n\nNoteNuclear Norm Properties\n\n\n\nThe nuclear norm plays a role analogous to the \\(\\ell_1\\) norm in (Equation 2.6):\n\nThe \\(\\ell_1\\) norm favors sparse vectors\nThe nuclear norm favors low-rank matrices\n\n\n\nAlthough the nuclear norm is a somewhat complex nonsmooth function, it is at least convex so that the formulation (Equation 2.8) is also convex.\nThis formulation can be shown to yield a statistically valid solution when:\n\nThe true \\(X\\) is low rank\nThe observation matrices \\(A_j\\) satisfy a “restricted isometry property” (commonly satisfied by random matrices but not by matrices with just one nonzero element)\n\nThe formulation is also valid in a different context, in which:\n\nThe true \\(X\\) is incoherent (roughly speaking, it does not have a few elements that are much larger than the others)\nThe observations \\(A_j\\) are of single elements\n\n\n\n2.4.3 Factorized Representation\nIn another form of regularization, the matrix \\(X\\) is represented explicitly as a product of two “thin” matrices \\(L\\) and \\(R\\), where \\(L \\in \\mathbb{R}^{n \\times r}\\) and \\(R \\in \\mathbb{R}^{p \\times r}\\), with \\(r \\ll \\min(n,p)\\). We set \\(X = LR^T\\) in (Equation 2.7) and solve:\n\\[\\min_{L,R} \\frac{1}{2m} \\sum_{j=1}^{m} (\\langle A_j, LR^T \\rangle - y_j)^2 \\tag{2.9}\\]\nIn this formulation:\n\nThe rank \\(r\\) is “hard-wired” into the definition of \\(X\\), so there is no need to include a regularizing term\nThis formulation is typically much more compact than (Equation 2.8); the total number of elements in \\((L,R)\\) is \\((n + p)r\\), which is much less than \\(np\\)\nHowever, this function is nonconvex when considered as a function of \\((L,R)\\) jointly\n\n\n\n\n\n\n\nImportantBenign Nonconvexity\n\n\n\nAn active line of current research shows that the nonconvexity is benign in many situations and that, under certain assumptions on the data \\((A_j,y_j)\\), \\(j = 1,2, \\ldots, m\\) and careful choice of algorithmic strategy, good solutions can be obtained from the formulation (Equation 2.9).\n\n\nA clue to this good behavior is that although this formulation is nonconvex, it is in some sense an approximation to a tractable problem: If we have a complete observation of \\(X\\), then a rank-\\(r\\) approximation can be found by performing a singular value decomposition of \\(X\\) and defining \\(L\\) and \\(R\\) in terms of the \\(r\\) leading left and right singular vectors.\n\n\n2.4.4 Nonnegative Matrix Factorization\nSome applications in computer vision, chemometrics, and document clustering require us to find factors \\(L\\) and \\(R\\) like those in (Equation 2.9) in which all elements are nonnegative. If the full matrix \\(Y \\in \\mathbb{R}^{n \\times p}\\) is observed, this problem has the form:\n\\[\\min_{L,R} \\|LR^T - Y\\|_F^2 \\quad \\text{subject to } L \\geq 0, \\; R \\geq 0 \\tag{2.10}\\]\nand is called nonnegative matrix factorization.",
    "crumbs": [
      "Optimization Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Optimization for Data Science</span>"
    ]
  },
  {
    "objectID": "chapters/optimization/01_intro.html#sec-svm",
    "href": "chapters/optimization/01_intro.html#sec-svm",
    "title": "2  Introduction to Optimization for Data Science",
    "section": "2.5 Support Vector Machines",
    "text": "2.5 Support Vector Machines\n\n\n\n\n\n\nNoteClassical ML Problem\n\n\n\nClassification via Support Vector Machines (SVM) is a classical optimization problem in machine learning, tracing its origins to the 1960s.\n\n\nGiven the input data \\((a_j,y_j)\\) with \\(a_j \\in \\mathbb{R}^n\\) and \\(y_j \\in \\{-1,1\\}\\), SVM seeks a vector \\(x \\in \\mathbb{R}^n\\) and a scalar \\(\\beta \\in \\mathbb{R}\\) such that:\n\\[a_j^T x - \\beta \\geq 1 \\quad \\text{when } y_j = +1 \\tag{2.11}\\]\n\\[a_j^T x - \\beta \\leq -1 \\quad \\text{when } y_j = -1 \\tag{2.12}\\]\nAny pair \\((x,\\beta)\\) that satisfies these conditions defines a separating hyperplane in \\(\\mathbb{R}^n\\), that separates the “positive” cases \\(\\{a_j \\mid y_j = +1\\}\\) from the “negative” cases \\(\\{a_j \\mid y_j = -1\\}\\).\nAmong all separating hyperplanes, the one that minimizes \\(\\|x\\|_2\\) is the one that maximizes the margin between the two classes – that is, the hyperplane whose distance to the nearest point \\(a_j\\) of either class is greatest.\n\n2.5.1 Hinge Loss Formulation\nWe can formulate the problem of finding a separating hyperplane as an optimization problem by defining an objective with the summation form (Equation 2.2):\n\\[H(x,\\beta) = \\frac{1}{m} \\sum_{j=1}^{m} \\max(1 - y_j(a_j^T x - \\beta), 0) \\tag{2.13}\\]\nNote that the \\(j\\)-th term in this summation is zero if the conditions (Equation 2.11)–(Equation 2.12) are satisfied, and it is positive otherwise. Even if no pair \\((x,\\beta)\\) exists for which \\(H(x,\\beta) = 0\\), a value \\((x,\\beta)\\) that minimizes (Equation 2.13) will be the one that comes as close as possible to satisfying the conditions in some sense.\nA term \\(\\frac{1}{2\\lambda}\\|x\\|_2^2\\) (for some parameter \\(\\lambda &gt; 0\\)) is often added to (Equation 2.13), yielding the following regularized version:\n\\[H(x,\\beta) = \\frac{1}{m} \\sum_{j=1}^{m} \\max(1 - y_j(a_j^T x - \\beta), 0) + \\frac{1}{2\\lambda}\\|x\\|_2^2 \\tag{2.14}\\]\n\n\n\n\n\n\nNoteLoss vs Regularizer\n\n\n\nIn contrast to the examples presented so far, the SVM problem has a nonsmooth loss function and a smooth regularizer.\n\n\nIf \\(\\lambda\\) is sufficiently small, and if separating hyperplanes exist, the pair \\((x,\\beta)\\) that minimizes (Equation 2.14) is the maximum-margin separating hyperplane. The maximum-margin property is consistent with the goals of generalizability and robustness.\nFor example, if the observed data \\((a_j,y_j)\\) is drawn from an underlying “cloud” of positive and negative cases, the maximum-margin solution usually does a reasonable job of separating other empirical data samples drawn from the same clouds, whereas a hyperplane that passes close to several of the observed data points may not do as well.\n\n\n2.5.2 Kernel Methods\nOften, it is not possible to find a hyperplane that separates the positive and negative cases well enough to be useful as a classifier. One solution is to transform all of the raw data vectors \\(a_j\\) by some nonlinear mapping \\(\\psi\\) and then perform the support vector machine classification on the vectors \\(\\psi(a_j)\\), \\(j = 1,2, \\ldots, m\\). The conditions (Equation 2.11)–(Equation 2.12) would thus be replaced by:\n\\[\\psi(a_j)^T x - \\beta \\geq 1 \\quad \\text{when } y_j = +1 \\tag{2.15}\\]\n\\[\\psi(a_j)^T x - \\beta \\leq -1 \\quad \\text{when } y_j = -1 \\tag{2.16}\\]\nleading to the following analog of (Equation 2.14):\n\\[H(x,\\beta) = \\frac{1}{m} \\sum_{j=1}^{m} \\max(1 - y_j(\\psi(a_j)^T x - \\beta), 0) + \\frac{1}{2\\lambda}\\|x\\|_2^2 \\tag{2.17}\\]\nWhen transformed back to \\(\\mathbb{R}^m\\), the surface \\(\\{a \\mid \\psi(a)^T x - \\beta = 0\\}\\) is nonlinear and possibly disconnected, and is often a much more powerful classifier than the hyperplanes resulting from (Equation 2.14).\n\n\n2.5.3 Dual Formulation\nWe note that SVM can also be expressed naturally as a minimization problem over a convex set. By introducing artificial variables, the problem (Equation 2.17) (and (Equation 2.14)) can be formulated as a convex quadratic program – that is, a problem with a convex quadratic objective and linear constraints.\nBy taking the dual of this problem, we obtain another convex quadratic program, in \\(m\\) variables:\n\\[\\begin{aligned}\n\\min_{\\alpha \\in \\mathbb{R}^m} \\quad & \\frac{1}{2}\\alpha^T Q\\alpha - \\mathbf{1}^T \\alpha \\\\\n\\text{subject to} \\quad & 0 \\leq \\alpha \\leq \\frac{1}{\\lambda}\\mathbf{1}, \\quad y^T \\alpha = 0\n\\end{aligned} \\tag{2.18}\\]\nwhere:\n\n\\(Q_{kl} = y_k y_l \\psi(a_k)^T \\psi(a_l)\\)\n\\(y = (y_1, y_2, \\ldots, y_m)^T\\)\n\n\\(\\mathbf{1} = (1, 1, \\ldots, 1)^T\\)\n\n\n\n\n\n\n\nImportantThe Kernel Trick\n\n\n\nInterestingly, problem (Equation 2.18) can be formulated and solved without explicit knowledge or definition of the mapping \\(\\psi\\). We need only a technique to define the elements of \\(Q\\). This can be done with the use of a kernel function \\(K : \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}\\), where \\(K(a_k,a_l)\\) replaces \\(\\psi(a_k)^T \\psi(a_l)\\). This is the so-called kernel trick.\n\n\nThe kernel function \\(K\\) can also be used to construct a classification function \\(\\phi\\) from the solution of (Equation 2.18). A particularly popular choice of kernel is the Gaussian kernel:\n\\[K(a_k,a_l) := \\exp\\left(-\\frac{1}{2\\sigma^2} \\|a_k - a_l\\|_2^2\\right) \\tag{2.19}\\]\nwhere \\(\\sigma\\) is a positive parameter.",
    "crumbs": [
      "Optimization Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Optimization for Data Science</span>"
    ]
  },
  {
    "objectID": "chapters/optimization/01_intro.html#sec-logistic-regression",
    "href": "chapters/optimization/01_intro.html#sec-logistic-regression",
    "title": "2  Introduction to Optimization for Data Science",
    "section": "2.6 Logistic Regression",
    "text": "2.6 Logistic Regression\nLogistic regression can be viewed as a softened form of binary support vector machine classification in which, rather than the classification function \\(\\phi\\) giving an unqualified prediction of the class in which a new data vector \\(a\\) lies, it returns an estimate of the odds of \\(a\\) belonging to one class or the other.\nWe seek an “odds function” \\(p\\) parametrized by a vector \\(x \\in \\mathbb{R}^n\\):\n\\[p(a;x) := (1 + \\exp(a^T x))^{-1} \\tag{2.20}\\]\nand aim to choose the parameter \\(x\\) so that:\n\n\\(p(a_j;x) \\approx 1\\) when \\(y_j = +1\\)\n\\(p(a_j;x) \\approx 0\\) when \\(y_j = -1\\)\n\nNote the similarity to (Equation 2.11)–(Equation 2.12).\n\n2.6.1 Negative Log-Likelihood\nThe optimal value of \\(x\\) can be found by minimizing a negative-log-likelihood function:\n\\[L(x) := -\\frac{1}{m} \\left[ \\sum_{j:y_j=-1} \\log(1 - p(a_j;x)) + \\sum_{j:y_j=1} \\log p(a_j;x) \\right] \\tag{2.21}\\]\nNote that the definition (Equation 2.20) ensures that \\(p(a;x) \\in (0,1)\\) for all \\(a\\) and \\(x\\); thus, \\(\\log(1 - p(a_j;x)) &lt; 0\\) and \\(\\log p(a_j;x) &lt; 0\\) for all \\(j\\) and all \\(x\\). When the conditions above are satisfied, these log terms will be only slightly negative, so values of \\(x\\) that satisfy them will be near optimal.\n\n\n2.6.2 Feature Selection with LASSO\nWe can perform feature selection using the model (Equation 2.21) by introducing a regularizer \\(\\lambda \\|x\\|_1\\) (as in the LASSO technique for least squares):\n\\[\\min_x -\\frac{1}{m} \\left[ \\sum_{j:y_j=-1} \\log(1 - p(a_j;x)) + \\sum_{j:y_j=1} \\log p(a_j;x) \\right] + \\lambda\\|x\\|_1 \\tag{2.22}\\]\nwhere \\(\\lambda &gt; 0\\) is a regularization parameter. This term has the effect of producing a solution in which few components of \\(x\\) are nonzero, making it possible to evaluate \\(p(a;x)\\) by knowing only those components of \\(a\\) that correspond to the nonzeros in \\(x\\).\n\n\n2.6.3 Multiclass Logistic Regression\nAn important extension of this technique is to multiclass (or multinomial) logistic regression, in which the data vectors \\(a_j\\) belong to more than two classes. Such applications are common in modern data analysis.\n\n\n\n\n\n\nTipExample: Speech Recognition\n\n\n\nIn a speech recognition system, the \\(M\\) classes could each represent a phoneme of speech, one of the potentially thousands of distinct elementary sounds that can be uttered by humans in a few tens of milliseconds.\n\n\nA multinomial logistic regression problem requires a distinct odds function \\(p_k\\) for each class \\(k \\in \\{1,2, \\ldots, M\\}\\). These functions are parametrized by vectors \\(x^{[k]} \\in \\mathbb{R}^n\\), \\(k = 1,2, \\ldots, M\\), defined as follows:\n\\[p_k(a;X) := \\frac{\\exp(a^T x^{[k]})}{\\sum_{l=1}^{M} \\exp(a^T x^{[l]})}, \\quad k = 1,2, \\ldots, M \\tag{2.23}\\]\nwhere we define \\(X := \\{x^{[k]} \\mid k = 1,2, \\ldots, M\\}\\).\nAs in the binary case, we have \\(p_k(a) \\in (0,1)\\) for all \\(a\\) and all \\(k = 1,2, \\ldots, M\\) and, in addition, that \\(\\sum_{k=1}^{M} p_k(a) = 1\\). The functions (Equation 2.23) perform a “softmax” on the quantities \\(\\{a^T x^{[l]} \\mid l = 1,2, \\ldots, M\\}\\).\nIn the setting of multiclass logistic regression, the labels \\(y_j\\) are vectors in \\(\\mathbb{R}^M\\) whose elements are defined as follows:\n\\[y_{jk} = \\begin{cases}\n1 & \\text{when } a_j \\text{ belongs to class } k, \\\\\n0 & \\text{otherwise.}\n\\end{cases} \\tag{2.24}\\]\nSimilarly to the binary case, we seek to define the vectors \\(x^{[k]}\\) so that:\n\n\\(p_k(a_j;X) \\approx 1\\) when \\(y_{jk} = 1\\)\n\\(p_k(a_j;X) \\approx 0\\) when \\(y_{jk} = 0\\)\n\nThe problem of finding values of \\(x^{[k]}\\) that satisfy these conditions can again be formulated as one of minimizing a negative-log-likelihood:\n\\[L(X) := -\\frac{1}{m} \\sum_{j=1}^{m} \\left[ \\sum_{\\ell=1}^{M} y_{j\\ell}(x^{[\\ell]T}a_j) - \\log \\left( \\sum_{\\ell=1}^{M} \\exp(x^{[\\ell]T}a_j) \\right) \\right] \\tag{2.25}\\]\n“Group-sparse” regularization terms can be included in this formulation to select a set of features in the vectors \\(a_j\\), common to each class, that distinguish effectively between the classes.",
    "crumbs": [
      "Optimization Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Optimization for Data Science</span>"
    ]
  },
  {
    "objectID": "chapters/optimization/01_intro.html#sec-deep-learning",
    "href": "chapters/optimization/01_intro.html#sec-deep-learning",
    "title": "2  Introduction to Optimization for Data Science",
    "section": "2.7 Deep Learning",
    "text": "2.7 Deep Learning\nDeep neural networks are often designed to perform the same function as multiclass logistic regression – that is, to classify a data vector \\(a\\) into one of \\(M\\) possible classes, often for large \\(M\\). The major innovation is that the mapping \\(\\phi\\) from data vector to prediction is now a nonlinear function, explicitly parametrized by a set of structured transformations.\n\n2.7.1 Neural Network Structure\nThe neural network shown in conceptual form illustrates the structure of a particular neural net. In this structure:\n\nThe data vector \\(a_j\\) enters at the left of the network\nEach box (more often referred to as a “layer”) represents a transformation that takes an input vector and applies a nonlinear transformation of the data to produce an output vector\nThe output of each operator becomes the input for one or more subsequent layers\nEach layer has a set of its own parameters, and the collection of all of the parameters over all the layers comprises our optimization variable\nThe different types of transformations might differ between layers, but we can compose them in whatever fashion suits our application\n\n\n\n2.7.2 Layer Transformations\nA typical transformation, which converts the vector \\(a_j^{l-1}\\) representing output from layer \\(l-1\\) to the vector \\(a_j^l\\) representing output from layer \\(l\\), is:\n\\[a_j^l = \\sigma(W^l a_j^{l-1} + g^l) \\tag{2.26}\\]\nwhere:\n\n\\(W^l\\) is a matrix of dimension \\(|a_j^l| \\times |a_j^{l-1}|\\)\n\\(g^l\\) is a vector of length \\(|a_j^l|\\)\n\\(\\sigma\\) is a componentwise nonlinear transformation, usually called an activation function\n\nThe most common forms of the activation function \\(\\sigma\\) act independently on each component of their argument vector as follows:\n\nSigmoid: \\(t \\to 1/(1 + e^{-t})\\)\nRectified Linear Unit (ReLU): \\(t \\to \\max(t,0)\\)\n\nAlternative transformations are needed when the input to box \\(l\\) comes from two or more preceding boxes.\n\n\n2.7.3 Output Layer and Loss Function\nThe rightmost layer of the neural network (the output layer) typically has \\(M\\) outputs, one for each of the possible classes to which the input (\\(a_j\\), say) could belong. These are compared to the labels \\(y_{jk}\\), defined as in (Equation 2.24) to indicate which of the \\(M\\) classes that \\(a_j\\) belongs to. Often, a softmax is applied to the outputs in the rightmost layer, and a loss function similar to (Equation 2.25) is obtained.\n\n\n2.7.4 Deep Learning Optimization Problem\nConsider the special (but not uncommon) case in which the neural net structure is a linear graph of \\(D\\) levels, in which the output for layer \\(l-1\\) becomes the input for layer \\(l\\) (for \\(l = 1,2, \\ldots, D\\)) with \\(a_j = a_j^0\\), \\(j = 1,2, \\ldots, m\\), and the transformation within each box has the form (Equation 2.26). A softmax is applied to the output of the rightmost layer to obtain a set of odds.\nThe parameters in this neural network are the matrix-vector pairs \\((W^l,g^l)\\), \\(l = 1,2, \\ldots, D\\) that transform the input vector \\(a_j = a_j^0\\) into the output \\(a_j^D\\) of the final layer. We aim to choose all these parameters so that the network does a good job of classifying the training data correctly.\nUsing the notation \\(w\\) for the layer-to-layer transformations, that is:\n\\[w := (W^1,g^1,W^2,g^2, \\ldots, W^D,g^D)\\]\nwe can write the loss function for deep learning as:\n\\[L(w) = -\\frac{1}{m} \\sum_{j=1}^{m} \\left[ \\sum_{\\ell=1}^{M} y_{j\\ell} a_{j,\\ell}^D(w) - \\log \\left( \\sum_{\\ell=1}^{M} \\exp a_{j,\\ell}^D(w) \\right) \\right] \\tag{2.27}\\]\nwhere \\(a_{j,\\ell}^D(w) \\in \\mathbb{R}\\) is the output of the \\(\\ell\\)-th element in layer \\(D\\) corresponding to input vector \\(a_j^0\\). (Here we write \\(a_{j,\\ell}^D(w)\\) to make explicit the dependence on the transformations \\(w\\) as well as on the input vector \\(a_j\\).)\nWe can view multiclass logistic regression as a special case of deep learning with \\(D = 1\\), so that \\(a_{j,\\ell}^1 = W_{\\ell,\\cdot}^1 a_j^0\\), where \\(W_{\\ell,\\cdot}^1\\) denotes row \\(\\ell\\) of the matrix \\(W^1\\).\n\n\n2.7.5 Variants and Engineering\nNeural networks in use for particular applications (for example, in image recognition and speech recognition, where they have been quite successful) include many variants on the basic design. These include:\n\nRestricted connectivity between the boxes (which corresponds to enforcing sparsity structure on the matrices \\(W^l\\), \\(l = 1,2, \\ldots, D\\))\nSharing parameters, which corresponds to forcing subsets of the elements of \\(W^l\\) to take the same value\nComplex arrangements of the boxes, with outputs coming from several layers, connections across nonadjacent layers, different componentwise transformations \\(\\sigma\\) at different layers, and so on\n\nDeep neural networks for practical applications are highly engineered objects.\n\n\n2.7.6 Distinctive Features of Deep Learning\nThe loss function (Equation 2.27) shares with many other applications the finite-sum form (Equation 2.2), but it has several features that set it apart from the other applications discussed before:\n\nNonconvexity: Most importantly, it is nonconvex in the parameters \\(w\\)\nLarge-scale: The total number of parameters in \\(w\\) is usually very large\n\nEffective training of deep learning classifiers typically requires a great deal of data and computation power. Huge clusters of powerful computers – often using multicore processors, GPUs, and even specially architected processing units – are devoted to this task.",
    "crumbs": [
      "Optimization Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Optimization for Data Science</span>"
    ]
  },
  {
    "objectID": "chapters/optimization/01_intro.html#sec-emphasis",
    "href": "chapters/optimization/01_intro.html#sec-emphasis",
    "title": "2  Introduction to Optimization for Data Science",
    "section": "2.8 Emphasis",
    "text": "2.8 Emphasis\nMany problems can be formulated as in the framework (Equation 2.3), and their properties may differ significantly. They might be convex or nonconvex, and smooth or nonsmooth. But there are important features that they all share:\n\n\n\n\n\n\nNoteShared Features of Optimization Problems\n\n\n\n\nThey can be formulated as functions of real variables, which we typically arrange in a vector of length \\(n\\)\nThe functions are continuous. When nonsmoothness appears in the formulation, it does so in a structured way that can be exploited by the algorithm\nSmoothness properties allow an algorithm to make good inferences about the behavior of the function on the basis of knowledge gained at nearby points that have been visited previously\nThe objective is often made up in part of a summation of many terms, where each term depends on a single item of data\nThe objective is often a sum of two terms: a “loss term” (sometimes arising from a maximum likelihood expression for some statistical model) and a “regularization term” whose purpose is to impose structure and “generalizability” on the recovered model\n\n\n\n\n2.8.1 Treatment Emphasis\nOur treatment emphasizes algorithms for solving these various kinds of problems, with analysis of the convergence properties of these algorithms. We pay attention to complexity guarantees, which are bounds on the amount of computational effort required to obtain solutions of a given accuracy. These bounds usually depend on fundamental properties of the objective function and the data that defines it, including:\n\nThe dimensions of the data set\nThe number of variables in the problem\n\nThis emphasis contrasts with much of the optimization literature, in which global convergence results do not usually involve complexity bounds. (A notable exception is the analysis of interior-point methods.)\n\n\n2.8.2 Practical Concerns\nAt the same time, we try as much as possible to emphasize the practical concerns associated with solving these problems. There are a variety of trade-offs presented by any problem, and the optimizer has to evaluate which tools are most appropriate to use. On top of the problem formulation, it is imperative to account for:\n\nThe time budget for the task at hand\nThe type of computer on which the problem will be solved\nThe guarantees needed for the solution",
    "crumbs": [
      "Optimization Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Optimization for Data Science</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "Summary & References",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Summary & References",
      "References"
    ]
  },
  {
    "objectID": "index.html#english-version",
    "href": "index.html#english-version",
    "title": "Data Analysis Collection",
    "section": "",
    "text": "Comprehensive collection of data analysis methods, optimization techniques, and machine learning fundamentals.\nTopics: - Optimization Methods - Statistical Analysis - Machine Learning\nGo to English Book →"
  },
  {
    "objectID": "index.html#versión-en-español",
    "href": "index.html#versión-en-español",
    "title": "Data Analysis Collection",
    "section": "",
    "text": "Colección completa de métodos de análisis de datos, técnicas de optimización y fundamentos de aprendizaje automático.\nTemas: - Métodos de Optimización - Análisis Estadístico - Aprendizaje Automático\nIr al Libro en Español →"
  }
]