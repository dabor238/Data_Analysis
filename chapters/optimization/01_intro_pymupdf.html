<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>

<meta charset="utf-8" />
<meta name="generator" content="quarto-1.8.26" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />

<meta name="author" content="PDF Conversion Tool" />

<title>Introduction to Optimization - PyMuPDF Version ‚Äì Data Analysis Collection</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>

<!-- htmldependencies:E3FAD763 -->
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="/index.html">
    <span class="navbar-title">Data Analysis Collection</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse"
  aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation"
  onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="/en/index.html"> 
<span class="menu-text">English</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="/es/index.html"> 
<span class="menu-text">Espa√±ol</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <div id="quarto-toc-target"></div>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Introduction to Optimization - PyMuPDF Version</h1>
<p class="subtitle lead">PyMuPDF Enhanced Conversion</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>PDF Conversion Tool </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">Invalid Date</p>
    </div>
  </div>
  
    
  </div>
  


</header>

<nav id="TOC" role="doc-toc">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-this-formulation-performs" id="toc-sec-this-formulation-performs"><span class="header-section-number">1</span> This formulation performs</a></li>
  <li><a href="#qkl-ykylœàakt-œàal-sec-qkl--ykylœàakt-œàal" id="toc-qkl-ykylœàakt-œàal-sec-qkl--ykylœàakt-œàal"><span class="header-section-number">2</span> Qkl = ykylœà(ak)T œà(al), {#sec-qkl-=-ykylœà(ak)t-œà(al),}</a></li>
  <li><a href="#kakal-exp-sec-kakal--exp" id="toc-kakal-exp-sec-kakal--exp"><span class="header-section-number">3</span> K(ak,al) := exp {#sec-k(ak,al)-:=-exp}</a></li>
  <li><a href="#sec-rm-whose-elements-are-deÔ¨Åned-as-follows:" id="toc-sec-rm-whose-elements-are-deÔ¨Åned-as-follows:"><span class="header-section-number">4</span> RM whose elements are deÔ¨Åned as follows:</a></li>
  </ul>
</nav>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span><strong>Conversion Information</strong>
</div>
</div>
<div class="callout-body-container callout-body">
<p>This document was converted from PDF using <strong>PyMuPDF</strong> as an alternative to Nougat OCR.</p>
<p><strong>Processing Notes:</strong> - Basic mathematical expression detection applied - Section headers automatically identified<br />
- Manual review recommended for complex equations - Enhanced formatting may be needed for optimal results</p>
</div>
</div>
<p>1 Introduction This book is about the fundamentals of algorithms for solving continuous optimization problems, which involve minimizing functions of multiple real- valued variables, possibly subject to some restrictions or constraints on the values that those variables may take. We focus particularly (though not exclusively) on convex problems, and our choice of topics is motivated by relevance to data science. That is, the formulations and algorithms that we discuss are useful in solving problems from machine learning, statistics, and data analysis. To set the stage for subsequent chapters, the rest of this chapter outlines several paradigms from data science and shows how they can be formulated as continuous optimization problems. We must pay attention to particular properties of these formulations ‚Äì their smoothness properties and structure ‚Äì when we choose algorithms to solve them. 1.1 Data Analysis and Optimization The typical optimization problem in data analysis is to Ô¨Ånd a model that agrees with some collected data set but also adheres to some structural constraints that reÔ¨Çect our beliefs about what a good model should be. The data set in a typical analysis problem consists of m objects: D := {(aj,yj), j = 1,2, . . . ,m}, (1.1) where aj is a vector (or matrix) of features and yj is an observation or label. (We can assume that the data has been cleaned so that all pairs (aj,yj), j = 1,2, . . . ,m have the same size and shape.) The data analysis task then consists of discovering a function œÜ such that œÜ(aj) ‚âàyj for most j = 1,2, . . . ,m. The process of discovering the mapping œÜ is often called ‚Äúlearning‚Äù or ‚Äútraining.‚Äù 1 https://doi.org/10.1017/9781009004282.002 Published online by Cambridge University Press</p>
<p>2 1 Introduction The function œÜ is often deÔ¨Åned in terms of a vector or matrix of parameters, which we denote in what follows by x or X (and occasionally by other notation). With these parametrizations, the problem of identifying œÜ becomes a traditional data-Ô¨Åtting problem: Find the parameters x deÔ¨Åning œÜ such that œÜ(aj) ‚âàyj, j = 1,2, . . . ,m in some optimal sense. Once we come up with a deÔ¨Ånition of the term ‚Äúoptimal‚Äù (and possibly also with restrictions on the values that we allow to parameters to take), we have an optimization problem. Frequently, these optimization formulations have objective functions of the Ô¨Ånite-sum type LD(x) := 1 m m  j=1 ‚Ñì(aj,yj;x). (1.2) The function ‚Ñì(a,y;x) here represents a ‚Äúloss‚Äù incurred for not properly aligning our prediction œÜ(a) with y. Thus, the objective LD(x) measures the average loss accrued over the entire data set when the parameter vector is equal to x. Once an appropriate value of x (and thus œÜ) has been learned from the data, we can use it to make predictions about other items of data not in the set D (1.1). Given an unseen item of data ÀÜa of the same type as aj, j = 1,2, . . . ,m, we predict the label ÀÜy associated with ÀÜa to be œÜ(ÀÜa). The mapping œÜ may also expose other structures and properties in the data set. For example, it may reveal that only a small fraction of the features in aj are needed to reliably predict the label yj. (This is known as feature selection.) When the parameter x is a matrix, it could reveal a low-dimensional subspace that contains most of the vectors aj, or it could reveal a matrix with particular structure (low-rank, sparse) such that observations of X prompted by the feature vectors aj yield results close to yj. The form of the labels yj differs according to the nature of the data analysis problem. ‚Ä¢ If each yj is a real number, we typically have a regression problem. ‚Ä¢ When each yj is a label, that is, an integer drawn from the set {1,2, . . . ,M} indicating that aj belongs to one of M classes, this is a classiÔ¨Åcation problem. When M = 2, we have a binary classiÔ¨Åcation problem, whereas M &gt; 2 is multiclass classiÔ¨Åcation. (In data analysis problems arising in speech and image recognition, M can be very large, of the order of thousands or more.) ‚Ä¢ The labels yj may not even exist; the data set may contain only the feature vectors aj, j = 1,2, . . . ,m. There are still interesting data analysis problems associated with these cases. For example, we may wish to group https://doi.org/10.1017/9781009004282.002 Published online by Cambridge University Press</p>
<p>1.1 Data Analysis and Optimization 3 the aj into clusters (where the vectors within each cluster are deemed to be functionally similar) or identify a low-dimensional subspace (or a collection of low-dimensional subspaces) that approximately contains the aj. In such problems, we are essentially learning the labels yj alongside the function œÜ. For example, in a clustering problem, yj could represent the cluster to which aj is assigned. Even after cleaning and preparation, the preceding setup may contain many complications that need to be dealt with in formulating the problem in rigorous mathematical terms. The quantities (aj,yj) may contain noise or may be otherwise corrupted, and we would like the mapping œÜ to be robust to such errors. There may be missing data: Parts of the vectors aj may be missing, or we may not know all the labels yj. The data may be arriving in streaming fashion rather than being available all at once. In this case, we would learn œÜ in an online fashion. One consideration that arises frequently is that we wish to avoid overÔ¨Åtting the model to the data set D in (1.1). The particular data set D available to us can often be thought of as a Ô¨Ånite sample drawn from some underlying larger (perhaps inÔ¨Ånite) collection of possible data points, and we wish the function œÜ to perform well on the unobserved data points as well as the observed subset D. In other words, we want œÜ to be not too sensitive to the particular sample D that is used to deÔ¨Åne empirical objective functions such as (1.2). One way to avoid this issue is to modify the objective function by adding constraints or penalty terms, in a way that limits the ‚Äúcomplexity‚Äù of the function œÜ. This process is typically called regularization. An optimization formulation that balances Ô¨Åt to the training data D, model complexity, and model structure is <span class="math display">\[\min_{\1}\]</span><span class="math inline">\(\in\)</span> LD(x) + Œª pen(x), (1.3) where  is a set of allowable values for x, pen(¬∑) is a regularization function or regularizer, and Œª ‚â•0 is a regularization parameter. The regularizer usually takes lower values for parameters x that yield functions œÜ with lower complex- ity. (For example, œÜ may depend on fewer of the features in the data vectors aj or may be less oscillatory.) The parameter Œª can be ‚Äútuned‚Äù to provide an appropriate balance between Ô¨Åtting the data and lowering the complexity of œÜ: Smaller values of Œª tend to produce solutions that Ô¨Åt the training data D more accurately, while large values of Œª lead to less complex models.1 1 Interestingly, the concept of overÔ¨Åtting has been reexamined in recent years, particularly in the context of deep learning, where models that perfectly Ô¨Åt the training data are sometimes observed to also do a good job of classifying previously unseen data. This phenomenon is a topic of intense current research in the machine learning community. https://doi.org/10.1017/9781009004282.002 Published online by Cambridge University Press</p>
<p>4 1 Introduction The constraint set  in (1.3) may be chosen to exclude values of x that are not relevant or useful in the context of the data analysis problem. For example, in some applications, we may not wish to consider values of x in which one or more components are negative, so we could set  to be the set of vectors whose components are all greater than or equal to zero. We now examine some particular problems in data science that give rise to formulations that are special cases of our master problem (1.3). We will see that a large variety of problems can be formulated using this general framework, but we will also see that within this framework, there is a wide range of structures that must be taken into account in choosing algorithms to solve these problems efÔ¨Åciently. 1.2 Least Squares Probably the oldest and best-known data analysis problem is linear least squares. Here, the data points (aj,yj) lie in Rn √ó R, and we solve <span class="math display">\[\min_{\1}\]</span> 1 2m m  j=1  aT j x ‚àíyj 2 = 1 2m‚à•Ax ‚àíy‚à•2 2, (1.4) where A the matrix whose rows are aT j , j = 1,2, . . . ,m and y = (y1,y2, . . . ,ym)T . In the preceding terminology, the function œÜ is deÔ¨Åned by œÜ(a) := aT x. (We can introduce a nonzero intercept by adding an extra parameter Œ≤ <span class="math inline">\(\in\)</span>R and deÔ¨Åning œÜ(a) := aT x + Œ≤.) This formulation can be motivated statistically, as a maximum-likelihood estimate of x when the observations yj are exact but for independent identically distributed (i.i.d.) Gaussian noise. We can add a variety of penalty functions to this basic least squares problem to impose desirable structure on x and, hence, on œÜ. For example, ridge regression adds a squared ‚Ñì2-norm penalty, resulting in <span class="math display">\[\min_{\1}\]</span> 1 2m‚à•Ax ‚àíy‚à•2 2 + Œª‚à•x‚à•2 2, for some parameter Œª &gt; 0. The solution x of this regularized formulation has less sensitivity to perturba- tions in the data (aj,yj). The LASSO formulation <span class="math display">\[\min_{\1}\]</span> 1 2m‚à•Ax ‚àíy‚à•2 2 + Œª‚à•x‚à•1 (1.5) tends to yield solutions x that are sparse ‚Äì that is, containing relatively few nonzero components (Tibshirani, 1996).</p>
<section id="sec-this-formulation-performs" class="level2" data-number="1">
<h2 data-number="1"><span class="header-section-number">1</span> This formulation performs</h2>
<p>feature selection: The locations of the nonzero components in x reveal those https://doi.org/10.1017/9781009004282.002 Published online by Cambridge University Press</p>
<p>1.3 Matrix Factorization Problems 5 components of aj that are instrumental in determining the observation yj. Besides its statistical appeal ‚Äì predictors that depend on few features are potentially simpler and more comprehensible than those depending on many features ‚Äì feature selection has practical appeal in making predictions about future data. Rather than gathering all components of a new data vector ÀÜa, we need to Ô¨Ånd only the ‚Äúselected‚Äù features because only these are needed to make a prediction. The LASSO formulation (1.5) is an important prototype for many problems in data analysis in that it involves a regularization term Œª‚à•x‚à•1 that is non- smooth and convex but has relatively simple structure that can potentially be exploited by algorithms. 1.3 Matrix Factorization Problems There are a variety of data analysis problems that require estimating a low-rank matrix from some sparse collection of data. Such problems can be formulated as natural extension of least squares to problems in which the data aj are naturally represented as matrices rather than vectors. Changing notation slightly, we suppose that each Aj is an n√óp matrix, and we seek another n √ó p matrix X that solves min X 1 2m m  j=1 (<span class="math inline">\(\langle \1 \rangle\)</span>‚àíyj)2, (1.6) where <span class="math inline">\(\langle \1 \rangle\)</span>:= trace(AT B). Here we can think of the Aj as ‚Äúprobing‚Äù the unknown matrix X. Commonly considered types of observations are random linear combinations (where the elements of Aj are selected i.i.d. from some distribution) or single-element observations (in which each Aj has 1 in a single location and zeros elsewhere). A regularized version of (1.6), leading to solutions X that are low rank, is min X 1 2m m  j=1 (<span class="math inline">\(\langle \1 \rangle\)</span>‚àíyj)2 + Œª‚à•X‚à•‚àó, (1.7) where ‚à•X‚à•‚àóis the nuclear norm, which is the sum of singular values of X (Recht et al., 2010). The nuclear norm plays a role analogous to the ‚Ñì1 norm in (1.5), where as the ‚Ñì1 norm favors sparse vectors, the nuclear norm favors low- rank matrices. Although the nuclear norm is a somewhat complex nonsmooth function, it is at least convex so that the formulation (1.7) is also convex. This formulation can be shown to yield a statistically valid solution when the true https://doi.org/10.1017/9781009004282.002 Published online by Cambridge University Press</p>
<p>6 1 Introduction X is low rank and the observation matrices Aj satisfy a ‚Äúrestricted isometry property,‚Äù commonly satisÔ¨Åed by random matrices but not by matrices with just one nonzero element. The formulation is also valid in a different context, in which the true X is incoherent (roughly speaking, it does not have a few elements that are much larger than the others), and the observations Aj are of single elements (Cand`es and Recht, 2009). In another form of regularization, the matrix X is represented explicitly as a product of two ‚Äúthin‚Äù matrices L and R, where L <span class="math inline">\(\in\)</span>Rn√ór and R <span class="math inline">\(\in\)</span>Rp√ór, with r ‚â™min(n,p). We set X = LRT in (1.6) and solve min L,R 1 2m m  j=1 (<span class="math inline">\(\langle \1 \rangle\)</span>‚àíyj)2. (1.8) In this formulation, the rank r is ‚Äúhard-wired‚Äù into the deÔ¨Ånition of X, so there is no need to include a regularizing term. This formulation is also typically much more compact than (1.7); the total number of elements in (L,R) is (n + p)r, which is much less than np. However, this function is nonconvex when considered as a function of (L,R) jointly. An active line of current research, pioneered by Burer and Monteiro (2003) and also drawing on statistical sources, shows that the nonconvexity is benign in many situations and that, under certain assumptions on the data (Aj,yj), j = 1,2, . . . ,m and careful choice of algorithmic strategy, good solutions can be obtained from the formulation (1.8). A clue to this good behavior is that although this formulation is nonconvex, it is in some sense an approximation to a tractable problem: If we have a complete observation of X, then a rank-r approximation can be found by performing a singular value decomposition of X and deÔ¨Åning L and R in terms of the r leading left and right singular vectors. Some applications in computer vision, chemometrics, and document clus- tering require us to Ô¨Ånd factors L and R like those in (1.8) in which all elements are nonnegative. If the full matrix Y <span class="math inline">\(\in\)</span>Rn√óp is observed, this problem has the form min L,R ‚à•LRT ‚àíY‚à•2 F, subject to L ‚â•0, R ‚â•0 and is called nonnegative matrix factorization. 1.4 Support Vector Machines ClassiÔ¨Åcation via support vector machines (SVM) is a classical optimization problem in machine learning, tracing its origins to the 1960s. Given the input https://doi.org/10.1017/9781009004282.002 Published online by Cambridge University Press</p>
<p>1.4 Support Vector Machines 7 data (aj,yj) with aj <span class="math inline">\(\in\)</span>Rn and yj <span class="math inline">\(\in\)</span>{‚àí1,1}, SVM seeks a vector x <span class="math inline">\(\in\)</span>Rn and a scalar Œ≤ <span class="math inline">\(\in\)</span>R such that aT j x ‚àíŒ≤ ‚â•1 when yj = +1, (1.9a) aT j x ‚àíŒ≤ ‚â§‚àí1 when yj = ‚àí1. (1.9b) Any pair (x,Œ≤) that satisÔ¨Åes these conditions deÔ¨Ånes a separating hyperplane in Rn, that separates the ‚Äúpositive‚Äù cases {aj | yj = +1} from the ‚Äúnegative‚Äù cases {aj | yj = ‚àí1}. Among all separating hyperplanes, the one that minimizes ‚à•x‚à•2 is the one that maximizes the margin between the two classes ‚Äì that is, the hyperplane whose distance to the nearest point aj of either class is greatest. We can formulate the problem of Ô¨Ånding a separating hyperplane as an optimization problem by deÔ¨Åning an objective with the summation form (1.2): H(x,Œ≤) = 1 m m  j=1 max(1 ‚àíyj(aT j x ‚àíŒ≤),0). (1.10) Note that the jth term in this summation is zero if the conditions (1.9) are satisÔ¨Åed, and it is positive otherwise. Even if no pair (x,Œ≤) exists for which H(x,Œ≤) = 0, a value (x,Œ≤) that minimizes (1.2) will be the one that comes as close as possible to satisfying (1.9) in some sense. A term Œª‚à•x‚à•2 2 (for some parameter Œª &gt; 0) is often added to (1.10), yielding the following regularized version: H(x,Œ≤) = 1 m m  j=1 max(1 ‚àíyj(aT j x ‚àíŒ≤),0) + 1 2Œª‚à•x‚à•2 2. (1.11) Note that, in contrast to the examples presented so far, the SVM problem has a nonsmooth loss function and a smooth regularizer. If Œª is sufÔ¨Åciently small, and if separating hyperplanes exist, the pair (x,Œ≤) that minimizes (1.11) is the maximum-margin separating hyperplane. The maximum-margin property is consistent with the goals of generalizability and robustness. For example, if the observed data (aj,yj) is drawn from an underlying ‚Äúcloud‚Äù of positive and negative cases, the maximum-margin solution usually does a reasonable job of separating other empirical data samples drawn from the same clouds, whereas a hyperplane that passes close to several of the observed data points may not do as well (see Figure 1.1). Often, it is not possible to Ô¨Ånd a hyperplane that separates the positive and negative cases well enough to be useful as a classiÔ¨Åer. One solution is to transform all of the raw data vectors aj by some nonlinear mapping œà and https://doi.org/10.1017/9781009004282.002 Published online by Cambridge University Press</p>
<p>8 1 Introduction . Figure 1.1 Linear support vector machine classiÔ¨Åcation, with the one class represented by circles and the other by squares. One possible choice of separating hyperplane is shown at left. If the training data is an empirical sample drawn from a cloud of underlying data points, this plane does not do well in separating the two clouds (middle). The maximum-margin separating hyperplane does better (right). then perform the support vector machine classiÔ¨Åcation on the vectors œà(aj), j = 1,2, . . . ,m. The conditions (1.9) would thus be replaced by œà(aj)T x ‚àíŒ≤ ‚â•1 when yj = +1; (1.12a) œà(aj)T x ‚àíŒ≤ ‚â§‚àí1 when yj = ‚àí1, (1.12b) leading to the following analog of (1.11): H(x,Œ≤) = 1 m m  j=1 max(1 ‚àíyj(œà(aj)T x ‚àíŒ≤),0) + 1 2Œª‚à•x‚à•2 2. (1.13) When transformed back to Rm, the surface {a | œà(a)T x ‚àíŒ≤ = 0} is nonlinear and possibly disconnected, and is often a much more powerful classiÔ¨Åer than the hyperplanes resulting from (1.11). We note that SVM can also be expressed naturally as a minimization problem over a convex set. By introducing artiÔ¨Åcial variables, the problem (1.13) (and (1.11)) can be formulated as a convex quadratic program ‚Äì that is, a problem with a convex quadratic objective and linear constraints. By taking the dual of this problem, we obtain another convex quadratic program, in m variables: min Œ±<span class="math inline">\(\in\)</span>Rm 1 2Œ±T QŒ± ‚àí1T Œ± subject to 0 ‚â§Œ± ‚â§1 Œª1, yT Œ± = 0, (1.14) where</p>
</section>
<section id="qkl-ykylœàakt-œàal-sec-qkl--ykylœàakt-œàal" class="level2" data-number="2">
<h2 data-number="2"><span class="header-section-number">2</span> Qkl = ykylœà(ak)T œà(al), {#sec-qkl-=-ykylœà(ak)t-œà(al),}</h2>
<p>y = (y1,y2, . . . ,ym)T, 1 = (1,1, . . . ,1)T . Interestingly, problem (1.14) can be formulated and solved without explicit knowledge or deÔ¨Ånition of the mapping œà. We need only a technique to deÔ¨Åne the elements of Q. This can be done with the use of a kernel function K : Rn √ó Rn ‚ÜíR, where K(ak,al) replaces œà(ak)T œà(al) (Boser et al., 1992; Cortes https://doi.org/10.1017/9781009004282.002 Published online by Cambridge University Press</p>
<p>1.5 Logistic Regression 9 and Vapnik, 1995). This is the so-called kernel trick. (The kernel function K can also be used to construct a classiÔ¨Åcation function œÜ from the solution of (1.14).) A particularly popular choice of kernel is the Gaussian kernel:</p>
</section>
<section id="kakal-exp-sec-kakal--exp" class="level2" data-number="3">
<h2 data-number="3"><span class="header-section-number">3</span> K(ak,al) := exp {#sec-k(ak,al)-:=-exp}</h2>
<p> ‚àí1 2œÉ ‚à•ak ‚àíal‚à•2  , where œÉ is a positive parameter. 1.5 Logistic Regression Logistic regression can be viewed as a softened form of binary support vector machine classiÔ¨Åcation in which, rather than the classiÔ¨Åcation function œÜ giving a unqualiÔ¨Åed prediction of the class in which a new data vector a lies, it returns an estimate of the odds of a belonging to one class or the other. We seek an ‚Äúodds function‚Äù p parametrized by a vector x <span class="math inline">\(\in\)</span>Rn, p(a;x) := (1 + exp(aT x))‚àí1, (1.15) and aim to choose the parameter x in so that p(aj;x) ‚âà1 when yj = +1; (1.16a) p(aj;x) ‚âà0 when yj = ‚àí1. (1.16b) (Note the similarity to (1.9).) The optimal value of x can be found by minimizing a negative-log-likelihood function: L(x) := ‚àí1 m ‚é° ‚é£ j:yj =‚àí1 log(1 ‚àíp(aj;x)) +  j:yj =1 log p(aj;x) ‚é§ ‚é¶. (1.17) Note that the deÔ¨Ånition (1.15) ensures that p(a;x) <span class="math inline">\(\in\)</span>(0,1) for all a and x; thus, log(1 ‚àíp(aj;x)) &lt; 0 and log p(aj;x) &lt; 0 for all j and all x. When the conditions (1.16) are satisÔ¨Åed, these log terms will be only slightly negative, so values of x that satisfy (1.17) will be near optimal. We can perform feature selection using the model (1.17) by introducing a regularizer Œª‚à•x‚à•1 (as in the LASSO technique for least squares (1.5)), <span class="math display">\[\min_{\1}\]</span> ‚àí1 m ‚é° ‚é£ j:yj =‚àí1 log(1 ‚àíp(aj;x)) +  j:yj =1 log p(aj;x) ‚é§ ‚é¶+ Œª‚à•x‚à•1, (1.18) where Œª &gt; 0 is a regularization parameter. As we see later, this term has the effect of producing a solution in which few components of x are nonzero, https://doi.org/10.1017/9781009004282.002 Published online by Cambridge University Press</p>
<p>10 1 Introduction making it possible to evaluate p(a;x) by knowing only those components of a that correspond to the nonzeros in x. An important extension of this technique is to multiclass (or multinomial) logistic regression, in which the data vectors aj belong to more than two classes. Such applications are common in modern data analysis. For example, in a speech recognition system, the M classes could each represent a phoneme of speech, one of the potentially thousands of distinct elementary sounds that can be uttered by humans in a few tens of milliseconds. A multinomial logistic regression problem requires a distinct odds function pk for each class k <span class="math inline">\(\in\)</span>{1,2, . . . ,M}. These functions are parametrized by vectors x[k] <span class="math inline">\(\in\)</span>Rn, k = 1,2, . . . ,M, deÔ¨Åned as follows: pk(a;X) := exp(aT x[k]) M l=1 exp(aT x[l]) , k = 1,2, . . . ,M, (1.19) where we deÔ¨Åne X := {x[k] | k = 1,2, . . . ,M}. As in the binary case, we have pk(a) <span class="math inline">\(\in\)</span>(0,1) for all a and all k = 1,2, . . . ,M and, in addition, that M k=1 pk(a) = 1. The functions (1.19) perform a ‚Äúsoftmax‚Äù on the quantities {aT x[l] | l = 1,2, . . . ,M}. In the setting of multiclass logistic regression, the labels yj are vectors in</p>
</section>
<section id="sec-rm-whose-elements-are-deÔ¨Åned-as-follows:" class="level2" data-number="4">
<h2 data-number="4"><span class="header-section-number">4</span> RM whose elements are deÔ¨Åned as follows:</h2>
<p>yjk =</p>
<p>1 when aj belongs to class k, 0 otherwise. (1.20) Similarly to (1.16), we seek to deÔ¨Åne the vectors x[k] so that pk(aj;X) ‚âà1 when yjk = 1 (1.21a) pk(aj;X) ‚âà0 when yjk = 0. (1.21b) The problem of Ô¨Ånding values of x[k] that satisfy these conditions can again be formulated as one of minimizing a negative-log-likelihood: L(X) := ‚àí1 m m  j=1 M  ‚Ñì=1 yj‚Ñì(xT [‚Ñì]aj) ‚àílog  M  ‚Ñì=1 exp(xT [‚Ñì]aj)  . (1.22) ‚ÄúGroup-sparse‚Äù regularization terms can be included in this formulation to select a set of features in the vectors aj, common to each class, that distinguish effectively between the classes. https://doi.org/10.1017/9781009004282.002 Published online by Cambridge University Press</p>
<p>1.6 Deep Learning 11 1.6 Deep Learning Deep neural networks are often designed to perform the same function as multiclass logistic regression ‚Äì that is, to classify a data vector a into one of M possible classes, often for large M. The major innovation is that the mapping œÜ from data vector to prediction is now a nonlinear function, explicitly parametrized by a set of structured transformations. The neural network shown in Figure 1.2 illustrates the structure of a particu- lar neural net. In this Ô¨Ågure, the data vector aj enters at the left of the network, and each box (more often referred to as a ‚Äúlayer‚Äù) represents a transformation that takes an input vector and applies a nonlinear transformation of the data to produce an output vector. The output of each operator becomes the input for one or more subsequent layers. Each layer has a set of its own parameters, and the collection of all of the parameters over all the layers comprises our optimization variable. The different shades of boxes here denote the fact that the types of transformations might differ between layers, but we can compose them in whatever fashion suits our application. A typical transformation, which converts the vector al‚àí1 j representing output from layer l ‚àí1 to the vector al j representing output from layer l, is al j = œÉ(W lal‚àí1 j + gl), (1.23) where W l is a matrix of dimension |al j|√ó|al‚àí1 j | and gl is a vector of length |al j|. The function œÉ is a componentwise nonlinear transformation, usually called an activation function. The most common forms of the activation function œÉ act independently on each component of their argument vector as follows: - Sigmoid: t ‚Üí1/(1 + e‚àít); - RectiÔ¨Åed Linear Unit (ReLU): t ‚Üímax(t,0). Alternative transformations are needed when the input to box l comes from two or more preceding boxes (as in the case for some boxes in Figure 1.2). The rightmost layer of the neural network (the output layer) typically has M outputs, one for each of the possible classes to which the input (aj, say) could belong. These are compared to the labels yjk, deÔ¨Åned as in (1.20) to indicate which of the M classes that aj belongs to. Often, a softmax is applied to the Figure 1.2 A deep neural network, showing connections between adjacent layers, where each layer is represented by a shaded rectangle. https://doi.org/10.1017/9781009004282.002 Published online by Cambridge University Press</p>
<p>12 1 Introduction outputs in the rightmost layer, and a loss function similar to (1.22) is obtained, as we describe now. Consider the special (but not uncommon) case in which the neural net structure is a linear graph of D levels, in which the output for layer l ‚àí1 becomes the input for layer l (for l = 1,2, . . . ,D) with aj = a0 j, j = 1,2, . . . ,m, and the transformation within each box has the form (1.23). A softmax is applied to the output of the rightmost layer to obtain a set of odds. The parameters in this neural network are the matrix-vector pairs (W l,gl), l = 1,2, . . . ,D that transform the input vector aj = a0 j into the output aD j of the Ô¨Ånal layer. We aim to choose all these parameters so that the network does a good job of classifying the training data correctly. Using the notation w for the layer-to-layer transformations, that is, w := (W 1,g1,W 2,g2, . . . ,W D,gD), we can write the loss function for deep learning as L(w) = ‚àí1 m m  j=1 M  ‚Ñì=1 yj‚ÑìaD j,‚Ñì(w) ‚àílog  M  ‚Ñì=1 exp aD j,‚Ñì(w)  , (1.24) where aD j,‚Ñì(w) <span class="math inline">\(\in\)</span>R is the output of the ‚Ñìth element in layer D corresponding to input vector a0 j. (Here we write aD j,‚Ñì(w) to make explicit the dependence on the transformations w as well as on the input vector aj.) We can view multiclass logistic regression as a special case of deep learning with D = 1, so that a1 j,‚Ñì= W 1 ‚Ñì,¬∑a0 j, where W 1 ‚Ñì,¬∑ denotes row ‚Ñìof the matrix W 1. Neural networks in use for particular applications (for example, in image recognition and speech recognition, where they have been quite successful) include many variants on the basic design. These include restricted connectiv- ity between the boxes (which corresponds to enforcing sparsity structure on the matrices W l, l = 1,2, . . . ,D) and sharing parameters, which corresponds to forcing subsets of the elements of W l to take the same value. Arrangements of the boxes may be quite complex, with outputs coming from several layers, con- nections across nonadjacent layers, different componentwise transformations œÉ at different layers, and so on. Deep neural networks for practical applications are highly engineered objects. The loss function (1.24) shares with many other applications the Ô¨Ånite-sum form (1.2), but it has several features that set it apart from the other applications discussed before. First, and possibly most important, it is nonconvex in the parameters w. Second, the total number of parameters in w is usually very large. Effective training of deep learning classiÔ¨Åers typically requires a great deal of data and computation power. Huge clusters of powerful computers ‚Äì https://doi.org/10.1017/9781009004282.002 Published online by Cambridge University Press</p>
<p>1.7 Emphasis 13 often using multicore processors, GPUs, and even specially architected pro- cessing units ‚Äì are devoted to this task. 1.7 Emphasis Many problems can be formulated as in the framework (1.3), and their properties may differ signiÔ¨Åcantly. They might be convex or nonconvex, and smooth or nonsmooth. But there are important features that they all share. ‚Ä¢ They can be formulated as functions of real variables, which we typically arrange in a vector of length n. ‚Ä¢ The functions are continuous. When nonsmoothness appears in the formulation, it does so in a structured way that can be exploited by the algorithm. Smoothness properties allow an algorithm to make good inferences about the behavior of the function on the basis of knowledge gained at nearby points that have been visited previously. ‚Ä¢ The objective is often made up in part of a summation of many terms, where each term depends on a single item of data. ‚Ä¢ The objective is often a sum of two terms: a ‚Äúloss term‚Äù (sometimes arising from a maximum likelihood expression for some statistical model) and a ‚Äúregularization term‚Äù whose purpose is to impose structure and ‚Äúgeneralizability‚Äù on the recovered model. Our treatment emphasizes algorithms for solving these various kinds of problems, with analysis of the convergence properties of these algorithms. We pay attention to complexity guarantees, which are bounds on the amount of computational effort required to obtain solutions of a given accuracy. These bounds usually depend on fundamental properties of the objective function and the data that deÔ¨Ånes it, including the dimensions of the data set and the number of variables in the problem. This emphasis contrasts with much of the optimization literature, in which global convergence results do not usually involve complexity bounds. (A notable exception is the analysis of interior- point methods (see Nesterov and Nemirovskii, 1994; Wright, 1997)). At the same time, we try as much as possible to emphasize the practical concerns associated with solving these problems. There are a variety of trade- offs presented by any problem, and the optimizer has to evaluate which tools are most appropriate to use. On top of the problem formulation, it is imperative to account for the time budget for the task at hand, the type of computer on which the problem will be solved, and the guarantees needed for the https://doi.org/10.1017/9781009004282.002 Published online by Cambridge University Press</p>
<div id="quarto-navigation-envelope" class="hidden">
<p><span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyLXRpdGxl">Data Analysis Collection</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXItdGl0bGU=">Data Analysis Collection</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6RW5nbGlzaA==">English</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L2VuL2luZGV4Lmh0bWw=">/en/index.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6RXNwYcOxb2w=">Espa√±ol</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L2VzL2luZGV4Lmh0bWw=">/es/index.html</span></p>
<div class="hidden quarto-markdown-envelope-contents" data-render-id="Zm9vdGVyLWxlZnQ=">
<p>¬© 2025 Norah Jones</p>
</div>
<div class="hidden quarto-markdown-envelope-contents" data-render-id="Zm9vdGVyLXJpZ2h0LUF2YWlsYWJsZSBpbiBFbmdsaXNoIGFuZCBTcGFuaXNo">
<p>Available in English and Spanish</p>
</div>
</div>
<div id="quarto-meta-markdown" class="hidden">
<p><span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW1ldGF0aXRsZQ==">Introduction to Optimization - PyMuPDF Version ‚Äì Data Analysis Collection</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLXR3aXR0ZXJjYXJkdGl0bGU=">Introduction to Optimization - PyMuPDF Version ‚Äì Data Analysis Collection</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW9nY2FyZHRpdGxl">Introduction to Optimization - PyMuPDF Version ‚Äì Data Analysis Collection</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW1ldGFzaXRlbmFtZQ==">Data Analysis Collection</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLXR3aXR0ZXJjYXJkZGVzYw==">PyMuPDF Enhanced Conversion</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW9nY2FyZGRkZXNj">PyMuPDF Enhanced Conversion</span></p>
</div>
</section>

</main> <!-- /main -->
<script id = "quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <div class='footer-contents'>¬© 2025 Norah Jones</div>  
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
      <i 
  class="bi bi-translate" 
  role="img" 
>
</i> 
 Available in English and Spanish
  </li>  
</ul>
    </div>
  </div>
</footer>

</body>

</html>
